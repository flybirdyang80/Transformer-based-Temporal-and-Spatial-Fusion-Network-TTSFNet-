{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71adb125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:47.388449Z",
     "start_time": "2024-05-09T02:56:47.377466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用 ssha,sswa,sssa,ssta 反演 次表层温度异常'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "使用 ssha,sswa,sssa,ssta 反演 次表层温度异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6decf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T07:56:19.232350Z",
     "start_time": "2025-05-18T07:56:17.552587Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:513: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#导包\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvLSTM2D,LSTM, BatchNormalization, LayerNormalization,Input, Conv3D, TimeDistributed, Flatten\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Concatenate,Conv2D,TimeDistributed, MaxPooling2D, Input, MaxPooling3D\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\__init__.py:46\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:96\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:140\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoShardPolicy\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalStatePolicy\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collections_abc \u001b[38;5;28;01mas\u001b[39;00m _collections_abc\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sorted\u001b[39m(dict_):\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:513\u001b[0m\n\u001b[0;32m    482\u001b[0m     _NP_TO_TF[pdt] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    483\u001b[0m         _NP_TO_TF[dt] \u001b[38;5;28;01mfor\u001b[39;00m dt \u001b[38;5;129;01min\u001b[39;00m _NP_TO_TF \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m==\u001b[39m pdt()\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    486\u001b[0m TF_VALUE_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(_NP_TO_TF\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    489\u001b[0m _TF_TO_NP \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    490\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF:\n\u001b[0;32m    491\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    492\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT:\n\u001b[0;32m    493\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    494\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE:\n\u001b[0;32m    495\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    496\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32:\n\u001b[0;32m    497\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    498\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8:\n\u001b[0;32m    499\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    500\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16:\n\u001b[0;32m    501\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    502\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32:\n\u001b[0;32m    503\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    504\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64:\n\u001b[0;32m    505\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    506\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16:\n\u001b[0;32m    507\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    508\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8:\n\u001b[0;32m    509\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING:\n\u001b[1;32m--> 513\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m,\n\u001b[0;32m    514\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64:\n\u001b[0;32m    515\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    516\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128:\n\u001b[0;32m    517\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    518\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64:\n\u001b[0;32m    519\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    520\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL:\n\u001b[0;32m    521\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool,\n\u001b[0;32m    522\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8:\n\u001b[0;32m    523\u001b[0m         _np_qint8,\n\u001b[0;32m    524\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8:\n\u001b[0;32m    525\u001b[0m         _np_quint8,\n\u001b[0;32m    526\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16:\n\u001b[0;32m    527\u001b[0m         _np_qint16,\n\u001b[0;32m    528\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16:\n\u001b[0;32m    529\u001b[0m         _np_quint16,\n\u001b[0;32m    530\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32:\n\u001b[0;32m    531\u001b[0m         _np_qint32,\n\u001b[0;32m    532\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16:\n\u001b[0;32m    533\u001b[0m         _np_bfloat16,\n\u001b[0;32m    534\u001b[0m \n\u001b[0;32m    535\u001b[0m     \u001b[38;5;66;03m# Ref types\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF_REF:\n\u001b[0;32m    537\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    538\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT_REF:\n\u001b[0;32m    539\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    540\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE_REF:\n\u001b[0;32m    541\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    542\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32_REF:\n\u001b[0;32m    543\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    544\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32_REF:\n\u001b[0;32m    545\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    546\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8_REF:\n\u001b[0;32m    547\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    548\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16_REF:\n\u001b[0;32m    549\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    550\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16_REF:\n\u001b[0;32m    551\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    552\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8_REF:\n\u001b[0;32m    553\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    554\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING_REF:\n\u001b[0;32m    555\u001b[0m         np\u001b[38;5;241m.\u001b[39mobject,\n\u001b[0;32m    556\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64_REF:\n\u001b[0;32m    557\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    558\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128_REF:\n\u001b[0;32m    559\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    560\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64_REF:\n\u001b[0;32m    561\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    562\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64_REF:\n\u001b[0;32m    563\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    564\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL_REF:\n\u001b[0;32m    565\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool,\n\u001b[0;32m    566\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8_REF:\n\u001b[0;32m    567\u001b[0m         _np_qint8,\n\u001b[0;32m    568\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8_REF:\n\u001b[0;32m    569\u001b[0m         _np_quint8,\n\u001b[0;32m    570\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16_REF:\n\u001b[0;32m    571\u001b[0m         _np_qint16,\n\u001b[0;32m    572\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16_REF:\n\u001b[0;32m    573\u001b[0m         _np_quint16,\n\u001b[0;32m    574\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32_REF:\n\u001b[0;32m    575\u001b[0m         _np_qint32,\n\u001b[0;32m    576\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16_REF:\n\u001b[0;32m    577\u001b[0m         _np_bfloat16,\n\u001b[0;32m    578\u001b[0m }\n\u001b[0;32m    580\u001b[0m _QUANTIZED_DTYPES_NO_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([qint8, quint8, qint16, quint16, qint32])\n\u001b[0;32m    581\u001b[0m _QUANTIZED_DTYPES_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[0;32m    582\u001b[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001b[1;32mD:\\tools\\anaconda\\envs\\python38\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "#导包\n",
    "from netCDF4 import Dataset\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import ConvLSTM2D,LSTM, BatchNormalization, LayerNormalization,Input, Conv3D, TimeDistributed, Flatten\n",
    "from tensorflow.keras.layers import Concatenate,Conv2D,TimeDistributed, MaxPooling2D, Input, MaxPooling3D\n",
    "from tensorflow.keras.layers import  Reshape,multiply\n",
    "from tensorflow.keras.layers import Layer,Lambda,Dot,ReLU, Dense, Dropout, Activation, Flatten,Attention\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.metrics import r2_score,accuracy_score,precision_score\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Sequential, regularizers\n",
    "# calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import gc \n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import match\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ebd19b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:50.801019Z",
     "start_time": "2024-05-09T02:56:50.675219Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查可用GPU数量\n",
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62d1f3",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353333a8",
   "metadata": {},
   "source": [
    "## SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cfdeb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:50.806011Z",
     "start_time": "2024-05-09T02:56:50.803016Z"
    }
   },
   "outputs": [],
   "source": [
    "extend_num =19  # 因为是移动滑块获取数据，所以要多取一些区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a10df6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:50.815995Z",
     "start_time": "2024-05-09T02:56:50.807010Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ssh1 = nc.Dataset(r'D:/data/SSH/SSH_2011-2015_month.nc')\n",
    "data_ssh2 = nc.Dataset(r'D:/data/SSH/SSH_2016-2021_month.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29342ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:50.823983Z",
     "start_time": "2024-05-09T02:56:50.816994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一段时间长度为： 60\n",
      "第二段时间长度为： 72\n"
     ]
    }
   ],
   "source": [
    "time_ssh1 = data_ssh1['time'][:]\n",
    "print('第一段时间长度为：',len(time_ssh1))\n",
    "time_ssh2 = data_ssh2['time'][:]\n",
    "print('第二段时间长度为：',len(time_ssh2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8338256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:50.828975Z",
     "start_time": "2024-05-09T02:56:50.824981Z"
    }
   },
   "outputs": [],
   "source": [
    "# 查看研究区域经纬度（120°E - 90°W , 0 - 60°N）\n",
    "ssh_lat = data_ssh1['lat'][:].data\n",
    "#print(ssh_lat[360:600])                   # 0.125 --- 59.875\n",
    "\n",
    "ssh_lon = data_ssh1['lon'][:].data\n",
    "# print(ssh_lon[720+480:])                 # 120.125 ---  179.875    240\n",
    "# print(ssh_lon[:360])                     #-179.875 ---  -90.125    360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db987a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.099544Z",
     "start_time": "2024-05-09T02:56:50.829973Z"
    }
   },
   "outputs": [],
   "source": [
    "ssh1 = np.concatenate((data_ssh1['ssh'][:,360-extend_num:600+extend_num,1200-extend_num:].data,data_ssh1['ssh'][:,360-extend_num:600+extend_num,:360+extend_num].data),axis=2)\n",
    "ssh2 = np.concatenate((data_ssh2['ssh'][:,360-extend_num:600+extend_num,1200-extend_num:].data,data_ssh2['ssh'][:,360-extend_num:600+extend_num,:360+extend_num].data),axis=2)\n",
    "#合并时间\n",
    "ssh =  np.concatenate((ssh1,ssh2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70caa2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.176426Z",
     "start_time": "2024-05-09T02:56:51.101541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "形状: (132, 278, 638)\n",
      "最大值与最小值： 2.266329 -0.37727273\n",
      "nan的个数： 43861\n"
     ]
    }
   ],
   "source": [
    "print('形状:',ssh.shape)\n",
    "print('最大值与最小值：',np.nanmax(ssh),np.nanmin(ssh))\n",
    "print('nan的个数：',np.sum(np.isnan(ssh[5:6,:,:]))) #31583"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ee64c",
   "metadata": {},
   "source": [
    "### SSHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e41dcb08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.298228Z",
     "start_time": "2024-05-09T02:56:51.183411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "海表面高度平均值 (278, 638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_100388\\1151614251.py:4: RuntimeWarning: Mean of empty slice\n",
      "  cli_ssh = np.nanmean(cli_ssh,axis = 0)\n"
     ]
    }
   ],
   "source": [
    "# 计算SSH 气候学\n",
    "cli_ssh = ssh  \n",
    "#cli_ssh = ssh.reshape(11, 12, 250, 610)    \n",
    "cli_ssh = np.nanmean(cli_ssh,axis = 0)\n",
    "print('海表面高度平均值',cli_ssh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41988522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.304218Z",
     "start_time": "2024-05-09T02:56:51.299226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 278, 638)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205dfa45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.338164Z",
     "start_time": "2024-05-09T02:56:51.305217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "#exp_cli_ssh = np.tile(cli_ssh, (11, 1, 1))\n",
    "ssha = ssh - cli_ssh \n",
    "print(ssha.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d2bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T09:21:49.138834Z",
     "start_time": "2023-11-12T09:21:49.135838Z"
    }
   },
   "source": [
    "## SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc48bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.344155Z",
     "start_time": "2024-05-09T02:56:51.339163Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sss = nc.Dataset(r'D:/data/SSS/2011-2021_SSS_correct.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e0801c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.349147Z",
     "start_time": "2024-05-09T02:56:51.345153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "# 确定时间\n",
    "time_sss = data_sss['time'][:]\n",
    "print(len(time_sss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722bf284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.354139Z",
     "start_time": "2024-05-09T02:56:51.350145Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取研究位置经纬度\n",
    "sss_lat = data_sss['lat'][:].data\n",
    "#print(sss_lat[360:600])                  # 0.125 ---- 59.875\n",
    "sss_lon = data_sss['lon'][:].data\n",
    "#print(sss_lon[480:480+600])              # 120.125 ----- 269.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c69839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.479939Z",
     "start_time": "2024-05-09T02:56:51.355137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638)\n",
      "(132, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "sss = data_sss['sos'][:,360-extend_num:600+extend_num,480-extend_num:1080+extend_num].data \n",
    "print(sss.shape)\n",
    "sss = np.squeeze(sss) # 移除大小为一的维度\n",
    "print(sss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26dca328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.554819Z",
     "start_time": "2024-05-09T02:56:51.480937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大值与最小值： 37.011566 18.755253\n",
      "nan的个数： 43592\n"
     ]
    }
   ],
   "source": [
    "print('最大值与最小值：',np.nanmax(sss),np.nanmin(sss))\n",
    "print('nan的个数：',np.sum(np.isnan(sss[5:6,:,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc8c20",
   "metadata": {},
   "source": [
    "### SSSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "585e1f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.714565Z",
     "start_time": "2024-05-09T02:56:51.555818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "海表面盐度平均值 (278, 638)\n",
      "(132, 278, 638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_100388\\2490087215.py:4: RuntimeWarning: Mean of empty slice\n",
      "  cli_sss = np.nanmean(cli_sss,axis = 0)\n"
     ]
    }
   ],
   "source": [
    "# 计算SSS 气候学\n",
    "cli_sss = sss  \n",
    "#cli_sss = sss.reshape(11, 12, 250, 610)    \n",
    "cli_sss = np.nanmean(cli_sss,axis = 0)\n",
    "print('海表面盐度平均值',cli_sss.shape)\n",
    "#exp_cli_sss = np.tile(cli_sss, (11, 1, 1))\n",
    "sssa = sss - cli_sss \n",
    "print(sssa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fffab1",
   "metadata": {},
   "source": [
    "## SSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4238b0f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.720556Z",
     "start_time": "2024-05-09T02:56:51.715564Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ssw = nc.Dataset(r'D:/data/SSW/2011-2021_SSW_correct.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b24ce8a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.725548Z",
     "start_time": "2024-05-09T02:56:51.721554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "# 确定时间\n",
    "time_ssw = data_ssw['time'][:132]\n",
    "print(len(time_ssw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffdb1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.730540Z",
     "start_time": "2024-05-09T02:56:51.726546Z"
    }
   },
   "outputs": [],
   "source": [
    "lat_ssw = data_ssw['lat'][:].data\n",
    "#print(lat_ssw[360:600])      # 0.125   ---- 59.875\n",
    "lon_ssw = data_ssw['lon'][:].data\n",
    "#print(lon_ssw[480:1080])      # 120.125  ---- 269.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d39b1c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.980143Z",
     "start_time": "2024-05-09T02:56:51.731538Z"
    }
   },
   "outputs": [],
   "source": [
    "uwnd = data_ssw['u'][:,360-extend_num:600+extend_num,480-extend_num:1080+extend_num].data  \n",
    "vwnd = data_ssw['v'][:,360-extend_num:600+extend_num,480-extend_num:1080+extend_num].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d52ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:51.988130Z",
     "start_time": "2024-05-09T02:56:51.984136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638)\n",
      "(132, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "print(uwnd.shape)\n",
    "print(vwnd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c705291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:52.055023Z",
     "start_time": "2024-05-09T02:56:51.989129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.203712, -13.459349, 9.947569, -14.963859)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmax(uwnd),np.nanmin(uwnd),np.nanmax(vwnd),np.nanmin(vwnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e60c244c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:52.061014Z",
     "start_time": "2024-05-09T02:56:52.056022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.isnan(uwnd[5:6,:,:])))\n",
    "print(np.sum(np.isnan(vwnd[5:6,:,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956bbad",
   "metadata": {},
   "source": [
    "### 风速异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd736e9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:52.341568Z",
     "start_time": "2024-05-09T02:56:52.062012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "海表面盐度平均值 (278, 638)\n",
      "(132, 278, 638)\n",
      "海表面盐度平均值 (278, 638)\n",
      "(132, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "# 计算uwnd  气候学\n",
    "cli_uwnd = uwnd  \n",
    "#cli_uwnd = uwnd.reshape(11, 12, 250, 610)    \n",
    "cli_uwnd = np.nanmean(cli_uwnd,axis = 0)\n",
    "print('海表面盐度平均值',cli_uwnd.shape)\n",
    "#exp_cli_uwnd = np.tile(cli_uwnd, (11, 1, 1))\n",
    "uwnda = uwnd - cli_uwnd \n",
    "print(uwnda.shape)\n",
    "\n",
    "# 计算vwnd  气候学v\n",
    "cli_vwnd = vwnd  \n",
    "#cli_vwnd = vwnd.reshape(11, 12, 250, 610)    \n",
    "cli_vwnd = np.nanmean(cli_vwnd,axis = 0)\n",
    "print('海表面盐度平均值',cli_vwnd.shape)\n",
    "#exp_cli_vwnd = np.tile(cli_vwnd, (11, 1, 1))\n",
    "vwnda = vwnd - cli_vwnd \n",
    "print(vwnda.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d70081",
   "metadata": {},
   "source": [
    "## 3DT  30-2000m温度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca5d086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:52.347558Z",
     "start_time": "2024-05-09T02:56:52.342566Z"
    }
   },
   "outputs": [],
   "source": [
    "data_3dt = nc.Dataset(r'D:/data/3DTS/2011-2021_3dt_correct.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67451b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:52.352551Z",
     "start_time": "2024-05-09T02:56:52.348556Z"
    }
   },
   "outputs": [],
   "source": [
    "# 确定区域(经纬度)\n",
    "lat_3dt = data_3dt['lat'][:].data\n",
    "#print(lat_3dt[329:329+240])        #0.125 --- 59.875\n",
    "lon_3dt = data_3dt['lon'][:].data    \n",
    "#print(lon_3dt[480:1080])            #120.125 --- 269.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "feaf19d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:58.616581Z",
     "start_time": "2024-05-09T02:56:52.353548Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_depth = data_3dt['to'][:,:,329-extend_num:569+extend_num,480-extend_num:1080+extend_num].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a7f7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:58.621573Z",
     "start_time": "2024-05-09T02:56:58.617579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 50, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "print(temp_depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0cde92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:58.627564Z",
     "start_time": "2024-05-09T02:56:58.622572Z"
    }
   },
   "outputs": [],
   "source": [
    "depths = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,80,90,100,125,150,175,200,225,250,275,300,350,400,\n",
    "         450,500,550,600,700,800,900,1000,1100,1200,1300,1400,1500,1750,2000]\n",
    "depths_use = [10,15,20,30,40,50,60,70,80,90,100,125,150,175,200,225,250,275,300,350,400,500,600,700,800,900,1000,1100,\n",
    "              1300,1500,1750,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef0b305b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:56:58.631557Z",
     "start_time": "2024-05-09T02:56:58.628562Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_depth_use = temp_depth[:,1:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8ba5d69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:26.390350Z",
     "start_time": "2024-05-09T02:56:58.632556Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,depth in enumerate(depths):\n",
    "    if(depth in depths_use):\n",
    "        #print(depths[i])\n",
    "        temp_depth_use = np.concatenate((temp_depth_use,temp_depth[:,i:i+1]),axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfeeeb6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:26.396341Z",
     "start_time": "2024-05-09T02:57:26.391350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间，深度，维度，经度 (132, 33, 278, 638)\n",
      "时间，维度，经度，深度 (132, 278, 638, 33)\n"
     ]
    }
   ],
   "source": [
    "print('时间，深度，维度，经度',temp_depth_use.shape)\n",
    "temp_depth_use = np.transpose(temp_depth_use, ( 0,2,3,1))\n",
    "print('时间，维度，经度，深度',temp_depth_use.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "026ab53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:26.402331Z",
     "start_time": "2024-05-09T02:57:26.397340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42510\n",
      "50221\n"
     ]
    }
   ],
   "source": [
    "# 查看不同深度中nan的总数。\n",
    "print(np.sum(np.isnan(temp_depth_use[5:6,:,:,0:1])))\n",
    "print(np.sum(np.isnan(temp_depth_use[5:6,:,:,14:15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1eb52",
   "metadata": {},
   "source": [
    "### 计算3DTA次表层温度异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "463f99af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:26.407324Z",
     "start_time": "2024-05-09T02:57:26.403330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 278, 638, 33)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_depth_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "485d3ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:30.612622Z",
     "start_time": "2024-05-09T02:57:26.408322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次表层温度平均值 (278, 638, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_100388\\2685273575.py:4: RuntimeWarning: Mean of empty slice\n",
      "  cli_3DT = np.nanmean(cli_3DT,axis = 0)\n"
     ]
    }
   ],
   "source": [
    "# 计算3DT 气候学\n",
    "cli_3DT = temp_depth_use   #(132, 250, 610, 15)\n",
    "#cli_3DT = temp_depth_use.reshape(11, 12, 250, 610, 33)    \n",
    "cli_3DT = np.nanmean(cli_3DT,axis = 0)\n",
    "print('次表层温度平均值',cli_3DT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aff6f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.946496Z",
     "start_time": "2024-05-09T02:57:30.613621Z"
    }
   },
   "outputs": [],
   "source": [
    "#exp_cli_3DT = np.tile(cli_3DT,(11,1,1,1))  #沿第一个轴重复11次，在其他轴上不进行重复\n",
    "A_3DT = temp_depth_use - cli_3DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12085c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.953486Z",
     "start_time": "2024-05-09T02:57:31.947495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = [5,10,15,20,30,40,50,60,70,80,90,100,125,150,175,200,225,250,275,300,350,400,500,600,700,800,900,1000,1100,\n",
    "              1300,1500,1750,2000]\n",
    "depth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0a44a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.958477Z",
     "start_time": "2024-05-09T02:57:31.954484Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def draw(time, depth ,vmin,vmax):\n",
    "# #     longitude = lon_3dt[480-5:1080+5] # 经度变量名\n",
    "# #     latitude = lat_3dt[329-5:329+240+5] # 纬度变量名\n",
    "#     longitude = lon_3dt[480:1080] # 经度变量名\n",
    "#     latitude = lat_3dt[329:329+240] # 纬度变量名\n",
    "\n",
    "#     # 选择第一个时间点的温度数据进行绘图\n",
    "#     sta_one = A_3DT[time,5:-5,5:-5,depth]\n",
    "\n",
    "#     # 创建一个图和坐标轴\n",
    "#     fig, ax = plt.subplots(figsize=(20, 15), subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "\n",
    "#     # 设置地图范围\n",
    "#     ax.set_extent([120,270, 0, 60], crs=ccrs.PlateCarree())\n",
    "\n",
    "#     # 添加网格线和标签\n",
    "#     gl = ax.gridlines(draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "#     gl.top_labels = False\n",
    "#     gl.right_labels = False\n",
    "#     gl.xlabel_style = {'size': 40}\n",
    "#     gl.ylabel_style = {'size': 40}\n",
    "\n",
    "#     # 绘制海岸线、国界\n",
    "#     ax.coastlines()\n",
    "#     ax.add_feature(cartopy.feature.BORDERS, linestyle=':')\n",
    "#     ax.add_feature(cartopy.feature.LAND, color='#dddddd')\n",
    "\n",
    "#     # 投影温度数据\n",
    "#     lon, lat = np.meshgrid(longitude, latitude)\n",
    "#     cs = ax.pcolormesh(lon, lat, np.squeeze(sta_one), cmap='jet', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree(),shading='auto')\n",
    "\n",
    "#     # 添加色标\n",
    "#     cbar = plt.colorbar(cs, orientation='vertical', pad=0.05, aspect=20,shrink=0.5)\n",
    "#     cbar.set_label('Temperature (°C)', fontsize=40)\n",
    "#     cbar.ax.tick_params(labelsize=35)\n",
    "\n",
    "\n",
    "#     plt.show()\n",
    "# draw(59, 30,-0.2,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101820e",
   "metadata": {},
   "source": [
    "## SST表面温度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24ea8dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.964468Z",
     "start_time": "2024-05-09T02:57:31.959476Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sst = nc.Dataset(r'D:/data/SST/SST_2011-2021_month_sheshidu_interp.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a869da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.969460Z",
     "start_time": "2024-05-09T02:57:31.965467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "# 确定时间\n",
    "time_sst = data_sst['time'][:]\n",
    "print(len(time_sst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "827769d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:31.974452Z",
     "start_time": "2024-05-09T02:57:31.970458Z"
    }
   },
   "outputs": [],
   "source": [
    "lat_sst = data_sst['lat'][:].data\n",
    "#print(lat_sst[360:600])             #0.125 --- 59.875\n",
    "lon_sst = data_sst['lon'][:].data\n",
    "# print(lon_sst[720+480:])             #120.125 --- 179.875\n",
    "# print(lon_sst[:360])                 #179.875 --- -90.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86433432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.241028Z",
     "start_time": "2024-05-09T02:57:31.975450Z"
    }
   },
   "outputs": [],
   "source": [
    "sst = np.concatenate((data_sst['sst'][:,360-extend_num:600+extend_num,1200-extend_num:].data,data_sst['sst'][:,360-extend_num:600+extend_num,:360+extend_num].data),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab261314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.317905Z",
     "start_time": "2024-05-09T02:57:32.242026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638)\n",
      "32.69934 -1.8612671\n",
      "43651\n"
     ]
    }
   ],
   "source": [
    "print(sst.shape)\n",
    "print(np.nanmax(sst),np.nanmin(sst))\n",
    "print(np.sum(np.isnan(sst[5:6,:,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7731357",
   "metadata": {},
   "source": [
    "### 计算STA表面温度异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c354b988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.441708Z",
     "start_time": "2024-05-09T02:57:32.318903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_100388\\343228742.py:4: RuntimeWarning: Mean of empty slice\n",
      "  cli_sst = np.nanmean(cli_sst,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# 计算sst 气候学\n",
    "cli_sst = sst\n",
    "#cli_sst = sst.reshape(11,12,250,610)\n",
    "cli_sst = np.nanmean(cli_sst,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68c2fe61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.446700Z",
     "start_time": "2024-05-09T02:57:32.442706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表面温度平均值 (278, 638)\n"
     ]
    }
   ],
   "source": [
    "print('表面温度平均值',cli_sst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6016185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.487634Z",
     "start_time": "2024-05-09T02:57:32.447698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638)\n"
     ]
    }
   ],
   "source": [
    "#exp_cli_sst = np.tile(cli_sst, (11, 1, 1))\n",
    "sta = sst - cli_sst \n",
    "print(sta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44f2e0",
   "metadata": {},
   "source": [
    "## 温度差值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbceab94",
   "metadata": {},
   "source": [
    "### 计算表面温度平均值与深度温度平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ad012f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.493625Z",
     "start_time": "2024-05-09T02:57:32.488633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 278, 638)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d932aa93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:32.498617Z",
     "start_time": "2024-05-09T02:57:32.494623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 278, 638, 33)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_depth_use[:120].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "940498a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:33.219468Z",
     "start_time": "2024-05-09T02:57:32.499615Z"
    }
   },
   "outputs": [],
   "source": [
    "#选取前10年的数据，将填充值替换为nan。\n",
    "mean_3DT = temp_depth_use[:120]\n",
    "mean_3DT[mean_3DT == 32767] = np.nan\n",
    "mean_sst = sst[:120]\n",
    "mean_sst[mean_sst == 32767] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce8df31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:37.913986Z",
     "start_time": "2024-05-09T02:57:33.220466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水下温度平均值： (12, 278, 638, 33)\n",
      "表面温度平均值： (12, 278, 638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_100388\\523653242.py:6: RuntimeWarning: Mean of empty slice\n",
      "  mean_3DT = np.nanmean(mean_3DT, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# 重构数组\n",
    "mean_3DT = mean_3DT.reshape(10, 12, 278, 638, 33)    \n",
    "mean_sst = mean_sst.reshape(10, 12, 278, 638)\n",
    "\n",
    "#沿年份维度（即第一个维度）计算平均值\n",
    "mean_3DT = np.nanmean(mean_3DT, axis=0)\n",
    "print('水下温度平均值：',mean_3DT.shape)\n",
    "\n",
    "mean_sst = np.mean(mean_sst, axis=0)\n",
    "print('表面温度平均值：',mean_sst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a9e8a",
   "metadata": {},
   "source": [
    "### 表面温度的平均值  减去 5-2000m温度的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f68e415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:38.401209Z",
     "start_time": "2024-05-09T02:57:37.914984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值形状： (12, 278, 638) (12, 278, 638, 33)\n",
      "(12, 278, 638, 33)\n"
     ]
    }
   ],
   "source": [
    "print('平均值形状：',mean_sst.shape,mean_3DT.shape)\n",
    "mean_sst = np.expand_dims(mean_sst,axis=-1)         #扩展维度 \n",
    "mean_sst = np.repeat(mean_sst, repeats=33,axis=-1)  #(12, 278, 638)  --》(12, 278, 638，33)  \n",
    "mean_different = mean_sst - mean_3DT\n",
    "print(mean_different.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382c2d1",
   "metadata": {},
   "source": [
    "### 得到温度异常差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad1cfb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:39.814956Z",
     "start_time": "2024-05-09T02:57:38.402208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638, 33)\n"
     ]
    }
   ],
   "source": [
    "repeated_mean_different = np.expand_dims(mean_different,axis=0)\n",
    "repeated_mean_different = np.tile(repeated_mean_different, (11, 1, 1, 1, 1))\n",
    "repeated_mean_different = repeated_mean_different.reshape(132, 278, 638,33)\n",
    "print(repeated_mean_different.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efceb2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:42.446760Z",
     "start_time": "2024-05-09T02:57:39.815954Z"
    }
   },
   "outputs": [],
   "source": [
    "repeated_sst = np.expand_dims(sst,axis=-1)\n",
    "repeated_sst = np.tile(repeated_sst, (1, 1, 1, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fae2cec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:43.595928Z",
     "start_time": "2024-05-09T02:57:42.447759Z"
    }
   },
   "outputs": [],
   "source": [
    "dsst = repeated_sst - repeated_mean_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e67ea1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:45.247295Z",
     "start_time": "2024-05-09T02:57:43.596926Z"
    }
   },
   "outputs": [],
   "source": [
    "dssta = dsst-cli_3DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f526892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:45.253286Z",
     "start_time": "2024-05-09T02:57:45.248294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 1100, 1300, 1500, 1750, 2000]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果将这些变量都作为模型的输入，则输入量过大。因此取了浅层，中层，深层的平均值，作为模型的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af2834e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:46.546224Z",
     "start_time": "2024-05-09T02:57:45.254284Z"
    }
   },
   "outputs": [],
   "source": [
    "dssta_lay1 = np.mean(dssta[:,:,:,:16],axis=-1)    #0-200m\n",
    "dssta_lay2 = np.mean(dssta[:,:,:,15:28],axis=-1)   #200-1000m\n",
    "dssta_lay3 = np.mean(dssta[:,:,:,27:],axis=-1)    #1000m-2000m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cda1619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:46.552215Z",
     "start_time": "2024-05-09T02:57:46.547223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132, 278, 638, 33), (132, 278, 638, 33))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsst.shape,temp_depth_use.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ea2b1",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248f79d",
   "metadata": {},
   "source": [
    "## 数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16a998ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:49.121119Z",
     "start_time": "2024-05-09T02:57:46.553213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638) (132, 278, 638) (132, 278, 638) (132, 278, 638) (132, 278, 638)\n",
      "表面数据： (132, 278, 638, 8)\n",
      "拼接后数据的形状： (132, 278, 638, 41)\n"
     ]
    }
   ],
   "source": [
    "# 合并数据方便统一处理。\n",
    "print(ssha.shape,sssa.shape,uwnda.shape,vwnda.shape,sta.shape)\n",
    "cat_surface =np.stack((ssha,sssa,uwnda,vwnda,sta,dssta_lay1,dssta_lay2,dssta_lay3),axis=3)\n",
    "print('表面数据：',cat_surface.shape)\n",
    "\n",
    "# 将表面数据和水下数据在最后一个维度拼接，将空值删除。\n",
    "cat_surface_depth = np.concatenate((cat_surface,A_3DT),axis=-1)\n",
    "print('拼接后数据的形状：',cat_surface_depth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114cc1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T11:42:58.855286Z",
     "start_time": "2023-11-12T11:42:58.851293Z"
    }
   },
   "source": [
    "## 统一形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3cf9f",
   "metadata": {},
   "source": [
    "### 塑形，（长\\*宽，时间\\*通道）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6f69cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:50.773484Z",
     "start_time": "2024-05-09T02:57:49.122117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 638, 132, 41)\n",
      "(177364, 5412)\n"
     ]
    }
   ],
   "source": [
    "## 先将合并的数据reshape成（长*宽，时间*通道）\n",
    "cat_surface_depth_reshape  = np.transpose(cat_surface_depth,(1,2,0,3))   # (132, 278, 638, 41) ---》 (278, 638, 132, 41)\n",
    "print(cat_surface_depth_reshape.shape)\n",
    "cat_surface_depth_reshape = cat_surface_depth_reshape.reshape ((-1,132*41))  # (278, 638, 132, 41) ---》(278*638, 132*41)\n",
    "print(cat_surface_depth_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80553601",
   "metadata": {},
   "source": [
    "### 一个点中只要有一个维度为nan,则将这个点的所有维度都设置为nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bfabce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:51.448408Z",
     "start_time": "2024-05-09T02:57:50.774482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_surface_depth_reshape: (177364, 5412)\n"
     ]
    }
   ],
   "source": [
    "# # 将填充值赋值为nan\n",
    "# cat_data_ss_depth_reshape[cat_data_ss_depth_reshape == 32767] = np.nan\n",
    "nan_mask = np.isnan(cat_surface_depth_reshape)\n",
    "\n",
    "# nan_mask.any(axis=1)==true,只要第一个维度（时间*通道）中含有nan就将所有设置为nan   (维度从0开始算起)\n",
    "cat_surface_depth_reshape[nan_mask.any(axis=1) == True] = np.nan   \n",
    "print('cat_surface_depth_reshape:',cat_surface_depth_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff31975",
   "metadata": {},
   "source": [
    "### 将处理好的数据转换回原始形状\n",
    "(177364, 5412) ——>  (132, 278, 638, 41) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acad1212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:51.454399Z",
     "start_time": "2024-05-09T02:57:51.449406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177364, 132, 41)\n",
      "(132, 177364, 41)\n",
      "(132, 278, 638, 41)\n"
     ]
    }
   ],
   "source": [
    "#转化形状\n",
    "cat_surface_depth_reshape = cat_surface_depth_reshape.reshape(-1,132,41)\n",
    "print(cat_surface_depth_reshape.shape)\n",
    "cat_surface_depth_reshape = np.transpose(cat_surface_depth_reshape,(1,0,2))\n",
    "print(cat_surface_depth_reshape.shape)\n",
    "cat_surface_depth_reshape = cat_surface_depth_reshape.reshape(-1,278, 638,41)\n",
    "print(cat_surface_depth_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d20aefe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:51.463384Z",
     "start_time": "2024-05-09T02:57:51.455397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58556"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(cat_surface_depth_reshape[5:6,:,:,2:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd84d7",
   "metadata": {},
   "source": [
    "## 将数据分成小块  [移动滑块获得数据]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "932d5117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:59.140140Z",
     "start_time": "2024-05-09T02:57:51.464382Z"
    }
   },
   "outputs": [],
   "source": [
    "# 在具有../model/lstm_transformer/mask/zt+ys_temp_mask.npy 文件的前提下，运行该行代码，否则要注释掉，\n",
    "#在后续代码获得zt+ys_temp_mask.npy之后，再重新运行全部代码即可。\n",
    "#当前已有zt+ys_temp_mask.npy文件直接运行即可。\n",
    "\n",
    "cat_surface_depth_reshape = np.nan_to_num(cat_surface_depth_reshape,nan=0)  #将所有Nan都设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d511709d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:59.146131Z",
     "start_time": "2024-05-09T02:57:59.141139Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 278, 638, 8) (132, 278, 638, 33)\n"
     ]
    }
   ],
   "source": [
    "# 表面数据\n",
    "cat_data_ss = cat_surface_depth_reshape[:,:,:,:8]\n",
    "# 深度数据\n",
    "cat_data_depth = cat_surface_depth_reshape[:,:,:,8:]\n",
    "print(cat_data_ss.shape,cat_data_depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66988643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:59.161110Z",
     "start_time": "2024-05-09T02:57:59.155117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.isnan(cat_data_depth[1:2,:,:,5:6])))\n",
    "# 查看差值的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70c8f238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:59.766142Z",
     "start_time": "2024-05-09T02:57:59.162105Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_data_ss_float16 = cat_data_ss.astype(np.float16)   #降低一下精度，是为了减小内存消耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71490059",
   "metadata": {},
   "source": [
    "### 移动滑块将数据处理成小块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76c9dc99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:57:59.778123Z",
     "start_time": "2024-05-09T02:57:59.767140Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 移动滑块\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "box_size = 39   # 滑块大小为39*39，通过隔行隔列取值得到的小块大小为20*20\n",
    "def move_slider():\n",
    "    time_size, lat_size, lon_size, channel = cat_data_ss.shape \n",
    "\n",
    "    # 定义矩形框的大小和步幅           \n",
    "    #box_size = 39  # 单位为度\n",
    "    step_size = 1  # 滑动步幅\n",
    "\n",
    "    # 计算输出网格的形状\n",
    "    out_lat_size = (lat_size - box_size) // step_size  + 1 \n",
    "    out_lon_size = (lon_size - box_size) // step_size  + 1 \n",
    "\n",
    "    print('time_size:',time_size,'lat_size:',lat_size,'lon_size:',lon_size)   \n",
    "    print('out_lat_size:',out_lat_size,'out_lon_size:',out_lon_size)  \n",
    "\n",
    "    #输出数组 \n",
    "    out_data = np.zeros((time_size, out_lat_size , out_lon_size, (box_size+1)//2, (box_size+1)//2, channel),dtype=np.float16) #(132, 240, 600, 9,9,5)\n",
    "    out_data_y = np.zeros((time_size, out_lat_size , out_lon_size, 33),dtype=np.float32) #(132, 272, 592,16)\n",
    "    out_data_lstm = np.zeros((time_size, out_lat_size , out_lon_size, channel),dtype=np.float32)\n",
    "\n",
    "    #========================CNN out_data =============================#\n",
    "    #遍历时间步、纬度和经度\n",
    "    for t in range(time_size):\n",
    "        sample_idx = 0\n",
    "        for lat in range(0, lat_size - box_size +1, step_size):\n",
    "            for lon in range(0, lon_size - box_size +1  , step_size):\n",
    "                # 计算当前矩形框的边界\n",
    "                lat_min = lat\n",
    "                lat_max = lat + box_size\n",
    "                lon_min = lon\n",
    "                lon_max = lon + box_size\n",
    "                #print(lat_min,lat_max,lon_min,lon_max)\n",
    "                # 获取当前矩形框的地图数据\n",
    "                box_data = cat_data_ss_float16[t, lat_min:lat_max:2, lon_min:lon_max:2,:]\n",
    "                out_data[t,lat,lon, :, :, :] = box_data\n",
    "                box_data = []\n",
    "                # 增加样本计数器\n",
    "                sample_idx += 1\n",
    "    print('out_data的形状',out_data.shape)\n",
    "\n",
    "\n",
    "    #======================LSTM 与 y out_data==========================#   \n",
    "\n",
    "    # 遍历时间步、纬度和经度\n",
    "    shift = (box_size-1)//2\n",
    "    for t in range(time_size):\n",
    "        sample_idx = 0\n",
    "        for lat in range(shift, lat_size-shift, step_size):\n",
    "            for lon in range(shift, lon_size-shift, step_size):\n",
    "                out_data_y[t,lat-shift,lon-shift, :] = cat_data_depth[t, lat:lat+1, lon:lon+1,:]\n",
    "                out_data_lstm[t,lat-shift,lon-shift, :] = cat_data_ss[t, lat:lat+1, lon:lon+1,:] \n",
    "                #print(sample_idx)\n",
    "                # 增加样本计数器\n",
    "                sample_idx += 1\n",
    "\n",
    "    print('out_data_y的形状',out_data_y.shape)\n",
    "    print('out_data_lstm的形状',out_data_lstm.shape)   \n",
    "    # out_data_reshape = slider_value(cat_data)\n",
    "\n",
    "\n",
    "    return out_data,out_data_y,out_data_lstm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adf5a943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.479285Z",
     "start_time": "2024-05-09T02:57:59.779121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_size: 132 lat_size: 278 lon_size: 638\n",
      "out_lat_size: 240 out_lon_size: 600\n",
      "out_data的形状 (132, 240, 600, 20, 20, 8)\n",
      "out_data_y的形状 (132, 240, 600, 33)\n",
      "out_data_lstm的形状 (132, 240, 600, 8)\n"
     ]
    }
   ],
   "source": [
    "out_data,out_data_y,out_data_lstm = move_slider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6995dcc",
   "metadata": {},
   "source": [
    "## 删除缺失值多于一半的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b33fe5",
   "metadata": {},
   "source": [
    "### 获取Nan_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7db04274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.484277Z",
     "start_time": "2024-05-09T02:59:55.480284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 144000, 20, 20, 8)\n"
     ]
    }
   ],
   "source": [
    "# 修改变量形状,修改成(时间，经度*维度，长，宽，通道)\n",
    "out_data_reshape = out_data.reshape((132,-1,(box_size+1)//2,(box_size+1)//2,8))\n",
    "print(out_data_reshape.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "702b4f63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.645020Z",
     "start_time": "2024-05-09T02:59:55.485276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144000, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "# 获取变量nan掩码\n",
    "nan_mask_out_data = np.isnan(out_data_reshape[0,:,:,:,0])\n",
    "print(nan_mask_out_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18e8ca66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.663990Z",
     "start_time": "2024-05-09T02:59:55.646019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(nan_mask_out_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c31e9b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.669980Z",
     "start_time": "2024-05-09T02:59:55.664988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144000, 400)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#其中true 代表Nan\n",
    "nan_mask_out_data_one = nan_mask_out_data.reshape(-1,(box_size+1)//2*(box_size+1)//2) \n",
    "nan_mask_out_data_one.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee08f65",
   "metadata": {},
   "source": [
    "#### 为有效值大于一半的方块标注True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d42dd20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:55.674973Z",
     "start_time": "2024-05-09T02:59:55.670979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nan_mask_out_data_one.shape[-1]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "517812d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.020420Z",
     "start_time": "2024-05-09T02:59:55.675971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_mask = [] \n",
    "for i in nan_mask_out_data_one:\n",
    "    if(np.count_nonzero(i) <=(nan_mask_out_data_one.shape[-1]//2)):    #True 代表nan，当nan小于等于61时，代表有效值大于一半\n",
    "        nan_mask.append(True)\n",
    "    else:\n",
    "        nan_mask.append(False)\n",
    "\n",
    "# 查看有效值个数\n",
    "np.count_nonzero(nan_mask)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e11a66",
   "metadata": {},
   "source": [
    "#### 标注lstm中的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2356a78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.026410Z",
     "start_time": "2024-05-09T02:59:56.021418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 240, 600, 8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_data_lstm.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b3b4225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.119276Z",
     "start_time": "2024-05-09T02:59:56.027409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 144000, 8)\n",
      "(132, 144000, 8)\n"
     ]
    }
   ],
   "source": [
    "## 标注lstm中的缺失值\n",
    "out_data_lstm_reshape = out_data_lstm.reshape((132,-1,8))\n",
    "print(out_data_lstm_reshape.shape)\n",
    "# 获取变量 nan 掩码\n",
    "nan_mask_lstm = np.isnan(out_data_lstm_reshape)\n",
    "print(nan_mask_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82e9b44d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.133263Z",
     "start_time": "2024-05-09T02:59:56.124293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(~nan_mask_lstm[0,:,3])     # 105773 这个数不变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4417e",
   "metadata": {},
   "source": [
    "#### 合并缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5fbb9201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.156202Z",
     "start_time": "2024-05-09T02:59:56.138268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144000\n"
     ]
    }
   ],
   "source": [
    "nan_mask_cat = [a and b for a, b in zip(nan_mask, ~nan_mask_lstm[0,:,0])]\n",
    "# 返回非零元素个数，即有效值个数\n",
    "print(np.count_nonzero(nan_mask_cat))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a2406f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.161195Z",
     "start_time": "2024-05-09T02:59:56.157201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nan_mask_cat = ~nan_mask_lstm[0,:,0]\n",
    "len(nan_mask_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a12c87",
   "metadata": {},
   "source": [
    "#### 保存数据/加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7e4d066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.167185Z",
     "start_time": "2024-05-09T02:59:56.162193Z"
    }
   },
   "outputs": [],
   "source": [
    "#！！若当前没有zt+ys_temp_mask.npy文件，则打开此注释，运行代码，运行后，注释该行代码，从头开始重新运行代码即可。\n",
    "#np.save(\"../model/lstm_transformer/mask/zt+ys_temp_mask.npy\",nan_mask_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0730256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.174174Z",
     "start_time": "2024-05-09T02:59:56.168183Z"
    }
   },
   "outputs": [],
   "source": [
    "nan_mask_cat = np.load(\"../model/lstm_transformer/mask/zt+ys_temp_mask.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5982c",
   "metadata": {},
   "source": [
    "### 修改数据形状，使其可以通过nan_mask_cat删除无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92474bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T02:59:56.181163Z",
     "start_time": "2024-05-09T02:59:56.175172Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_data_reshape: (144000, 132, 20, 20, 8)\n",
      "out_data_y_reshape: (240, 600, 132, 33)\n",
      "out_data_y_reshape: (144000, 132, 33)\n",
      "out_data_lstm_reshape: (240, 600, 132, 8)\n",
      "out_data_lstm_reshape: (144000, 132, 8)\n"
     ]
    }
   ],
   "source": [
    "# 重塑形状用于删除无效值\n",
    "out_data_reshape =  np.transpose(out_data_reshape,(1,0,2,3,4))\n",
    "print('out_data_reshape:',out_data_reshape.shape)\n",
    "\n",
    "out_data_y_reshape =  np.transpose(out_data_y,(1,2,0,3))\n",
    "print('out_data_y_reshape:',out_data_y_reshape.shape)\n",
    "out_data_y_reshape = out_data_y_reshape.reshape(-1,132,33)\n",
    "print('out_data_y_reshape:',out_data_y_reshape.shape)\n",
    "\n",
    "out_data_lstm_reshape =  np.transpose(out_data_lstm,(1,2,0,3))\n",
    "print('out_data_lstm_reshape:',out_data_lstm_reshape.shape)\n",
    "out_data_lstm_reshape = out_data_lstm_reshape.reshape(-1,132,8)\n",
    "print('out_data_lstm_reshape:',out_data_lstm_reshape.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c2d01",
   "metadata": {},
   "source": [
    "### 开始删除无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c76bfa7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:30.584112Z",
     "start_time": "2024-05-09T02:59:56.182161Z"
    }
   },
   "outputs": [],
   "source": [
    "out_data_reshape = out_data_reshape[nan_mask_cat]\n",
    "out_data_y_reshape = out_data_y_reshape[nan_mask_cat]\n",
    "out_data_lstm_reshape = out_data_lstm_reshape[nan_mask_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4dab603d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:41.089295Z",
     "start_time": "2024-05-09T03:00:30.585110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del out_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a59a2",
   "metadata": {},
   "source": [
    "### 使用0来填充Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71ec57be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:41.094288Z",
     "start_time": "2024-05-09T03:00:41.090294Z"
    }
   },
   "outputs": [],
   "source": [
    "# out_data_reshape = np.nan_to_num(out_data_reshape, nan=0)   # (103952, 132, 9, 9, 5) \n",
    "# out_data_y_reshape = np.nan_to_num(out_data_y_reshape, nan=0)  #(103952, 132, 15)\n",
    "# out_data_lstm_reshape = np.nan_to_num(out_data_lstm_reshape, nan=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0c0a4",
   "metadata": {},
   "source": [
    "# 验证集和测试集的划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55523f8",
   "metadata": {},
   "source": [
    "## 对数据进行reshape,以进行数据集的划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94f288cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:41.100278Z",
     "start_time": "2024-05-09T03:00:41.095286Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_data_reshape: (132, 103651, 20, 20, 8)\n",
      "out_data_y_reshape: (132, 103651, 33)\n",
      "out_data_lstm_reshape: (132, 103651, 8)\n"
     ]
    }
   ],
   "source": [
    "# 将时间放到前面\n",
    "out_data_reshape = np.transpose(out_data_reshape,(1,0,2,3,4))\n",
    "print('out_data_reshape:',out_data_reshape.shape)\n",
    "out_data_y_reshape =  np.transpose(out_data_y_reshape,(1,0,2))\n",
    "print('out_data_y_reshape:',out_data_y_reshape.shape)\n",
    "out_data_lstm_reshape =  np.transpose(out_data_lstm_reshape,(1,0,2))\n",
    "print('out_data_lstm_reshape:',out_data_lstm_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e51ef",
   "metadata": {},
   "source": [
    "## 将数据进行堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e5f03f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:47.024792Z",
     "start_time": "2024-05-09T03:00:41.101276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 12, 103651, 8)\n"
     ]
    }
   ],
   "source": [
    "available = len(out_data_lstm_reshape) #获取时间长度\n",
    "predict = 1 # 预测时间长度\n",
    "\n",
    "out_data_lstm_reshape_shape = out_data_lstm_reshape.shape[1:]\n",
    "N_stack = 12 #使用12个月的数据进行预测\n",
    "X_len = available - (N_stack-1)\n",
    "\n",
    "X_lstm = np.zeros(shape = (X_len, N_stack,out_data_lstm_reshape_shape[0],out_data_lstm_reshape_shape[1]))  \n",
    "for i in range(X_len):\n",
    "    X_lstm[i] = np.stack(out_data_lstm_reshape[i:i+N_stack])\n",
    "    #print(i)\n",
    "\n",
    "print(X_lstm.shape)\n",
    "#print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e55969",
   "metadata": {},
   "source": [
    "## 统一时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "17139f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:47.028788Z",
     "start_time": "2024-05-09T03:00:47.025791Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = out_data_y_reshape[11:]\n",
    "X_cnn = out_data_reshape[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ed2d1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:47.033778Z",
     "start_time": "2024-05-09T03:00:47.029785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 103651, 33)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f443c2",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c7084bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:00:47.039769Z",
     "start_time": "2024-05-09T03:00:47.034777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 103651, 20, 20, 8) (109, 12, 103651, 8) (109, 103651, 33) (12, 103651, 20, 20, 8) (12, 12, 103651, 8) (12, 103651, 33)\n"
     ]
    }
   ],
   "source": [
    "# 最后十二个月的数据作为test\n",
    "test_num = -12\n",
    "x_cnn_train = X_cnn[:test_num]\n",
    "x_lstm_train = X_lstm[:test_num]\n",
    "y_train = Y[:test_num]\n",
    "\n",
    "x_cnn_test = X_cnn[test_num:]\n",
    "x_lstm_test = X_lstm[test_num:]\n",
    "y_test = Y[test_num:]\n",
    "\n",
    "print(x_cnn_train.shape,x_lstm_train.shape,y_train.shape,x_cnn_test.shape,x_lstm_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1956c2c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:01:22.197463Z",
     "start_time": "2024-05-09T03:00:47.040767Z"
    }
   },
   "outputs": [],
   "source": [
    "#将前两个维度进行合并  时间*(lat*lon) = batchsize\n",
    "\n",
    "x_cnn_train = np.reshape(x_cnn_train,(-1,(box_size+1)//2,(box_size+1)//2,x_cnn_train.shape[-1]))\n",
    "x_cnn_test = np.reshape(x_cnn_test,(-1,(box_size+1)//2,(box_size+1)//2,x_cnn_train.shape[-1]))\n",
    "\n",
    "y_train = np.reshape(y_train,(-1,33))\n",
    "y_test = np.reshape(y_test,(-1,33))\n",
    "\n",
    "x_lstm_train = np.transpose(x_lstm_train, (0,2,1,3))  # (115, 6, 103623, 5)  ---> (115, 103623, 6, 5)\n",
    "x_lstm_train = x_lstm_train.reshape((-1,N_stack,8))          #(115, 103623, 6, 5)  ----->(115*103623, 6, 5)\n",
    "x_lstm_test = np.transpose(x_lstm_test, (0,2,1,3))\n",
    "x_lstm_test = x_lstm_test.reshape((-1,N_stack,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6d35745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:01:22.203454Z",
     "start_time": "2024-05-09T03:01:22.198462Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_lstm_train (11297959, 12, 8)\n",
      "x_cnn_train (11297959, 20, 20, 8)\n",
      "y_train (11297959, 33)\n",
      "x_cnn_test: (1243812, 20, 20, 8)\n",
      "x_lstm_test: (1243812, 12, 8)\n",
      "y_test (1243812, 33)\n"
     ]
    }
   ],
   "source": [
    "# 查看形状\n",
    "print('x_lstm_train',x_lstm_train.shape)\n",
    "print('x_cnn_train',x_cnn_train.shape)\n",
    "print('y_train',y_train.shape)\n",
    "\n",
    "print('x_cnn_test:',x_cnn_test.shape)\n",
    "print('x_lstm_test:',x_lstm_test.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49c22b",
   "metadata": {},
   "source": [
    "## 划分训练数据和目标数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cb18341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:09.608491Z",
     "start_time": "2024-05-09T03:01:22.204452Z"
    }
   },
   "outputs": [],
   "source": [
    "x_cnn_train_split, x_cnn_val, y_train_split, y_val = train_test_split(x_cnn_train , y_train, test_size=0.3, random_state=100)\n",
    "x_lstm_train_split, x_lstm_val, y_train_split_2, y_val_2 = train_test_split(x_lstm_train, y_train, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da724197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:09.614482Z",
     "start_time": "2024-05-09T03:02:09.609489Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cnn_train_split (7908571, 20, 20, 8)\n",
      "x_lstm_train_split (7908571, 12, 8)\n",
      "y_train_split (7908571, 33)\n",
      "x_cnn_test (1243812, 20, 20, 8)\n",
      "x_lstm_test (1243812, 12, 8)\n",
      "y_test (1243812, 33)\n",
      "x_cnn_val (3389388, 20, 20, 8)\n",
      "x_lstm_val (3389388, 12, 8)\n",
      "y_val (3389388, 33)\n"
     ]
    }
   ],
   "source": [
    "# 输出形状\n",
    "print('x_cnn_train_split',x_cnn_train_split.shape)\n",
    "print('x_lstm_train_split',x_lstm_train_split.shape)\n",
    "print('y_train_split',y_train_split_2.shape)\n",
    "\n",
    "print('x_cnn_test',x_cnn_test.shape)\n",
    "print('x_lstm_test',x_lstm_test.shape)\n",
    "print('y_test',y_test.shape)\n",
    "\n",
    "print('x_cnn_val',x_cnn_val.shape)\n",
    "print('x_lstm_val',x_lstm_val.shape)\n",
    "print('y_val',y_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fb13d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:16.360668Z",
     "start_time": "2024-05-09T03:02:09.615480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_cnn,out_data_reshape\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df89de",
   "metadata": {},
   "source": [
    "# 特征归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00456abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:16.366659Z",
     "start_time": "2024-05-09T03:02:16.361667Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 记录数据集的形状\n",
    "# train \n",
    "x_cnn_train_split_shape = x_cnn_train_split.shape\n",
    "x_lstm_train_split_shape = x_lstm_train_split.shape \n",
    "y_train_split_shape  = y_train_split_2.shape\n",
    "\n",
    "# test \n",
    "x_cnn_test_shape = x_cnn_test.shape\n",
    "x_lstm_test_shape = x_lstm_test.shape\n",
    "y_test_shape = y_test.shape\n",
    "\n",
    "#val\n",
    "x_cnn_val_shape = x_cnn_val.shape\n",
    "x_lstm_val_shape = x_lstm_val.shape\n",
    "y_val_shape = y_val_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "990adbf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:16.371650Z",
     "start_time": "2024-05-09T03:02:16.367657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cnn_train_split_shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2cb092a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:02:16.376642Z",
     "start_time": "2024-05-09T03:02:16.372649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cnn_train_split.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f9f23c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:08.271498Z",
     "start_time": "2024-05-09T03:02:16.377640Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 变量标准化\n",
    "scaler_f = StandardScaler()\n",
    "scaler_f_2 = StandardScaler()\n",
    "# 按照通道维度进行标准化\n",
    "x_cnn_train_split = scaler_f.fit_transform(x_cnn_train_split.reshape(-1,x_cnn_train_split_shape[3])) \n",
    "x_lstm_train_split = scaler_f_2.fit_transform(x_lstm_train_split.reshape(-1,x_lstm_train_split_shape[2])) \n",
    "\n",
    "x_cnn_test = scaler_f.transform(x_cnn_test.reshape(-1,x_cnn_test_shape[3]))\n",
    "x_lstm_test = scaler_f_2.transform(x_lstm_test.reshape(-1,x_lstm_test_shape[2]))\n",
    "\n",
    "x_cnn_val = scaler_f.transform(x_cnn_val.reshape(-1,x_cnn_val_shape[3]))\n",
    "x_lstm_val = scaler_f_2.transform(x_lstm_val.reshape(-1,x_lstm_val_shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9a86d6d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:13.855469Z",
     "start_time": "2024-05-09T03:16:08.272497Z"
    }
   },
   "outputs": [],
   "source": [
    "# 目标值标准化\n",
    "scaler_l = StandardScaler()\n",
    "# 按照深度进行标准化\n",
    "y_train_split = scaler_l.fit_transform(y_train_split_2.reshape(-1,y_train_split_shape[1])) \n",
    "y_test = scaler_l.transform(y_test.reshape(-1,y_test_shape[1])) \n",
    "y_val = scaler_l.transform(y_val_2.reshape(-1,y_val_shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e5e1f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:13.868448Z",
     "start_time": "2024-05-09T03:16:13.862458Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#reshape\n",
    "x_cnn_train_split = np.reshape(x_cnn_train_split,(-1,(box_size+1)//2,(box_size+1)//2,8))\n",
    "x_cnn_test = np.reshape(x_cnn_test,(-1,(box_size+1)//2,(box_size+1)//2,8))\n",
    "x_cnn_val = np.reshape(x_cnn_val,(-1,(box_size+1)//2,(box_size+1)//2,8))\n",
    "\n",
    "\n",
    "x_lstm_train_split = np.reshape(x_lstm_train_split,(-1,N_stack,8))\n",
    "x_lstm_test = np.reshape(x_lstm_test,(-1,N_stack,8))\n",
    "x_lstm_val = np.reshape(x_lstm_val,(-1,N_stack,8))\n",
    "\n",
    "\n",
    "y_train_split = np.reshape(y_train_split,(-1,33))\n",
    "y_test = np.reshape(y_test,(-1,33))\n",
    "y_val = np.reshape(y_val,(-1,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5afd48c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:13.874438Z",
     "start_time": "2024-05-09T03:16:13.869447Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cnn_train_split (7908571, 20, 20, 8)\n",
      "x_lstm_train_split (7908571, 12, 8)\n",
      "y_train_split (7908571, 33)\n",
      "x_cnn_test (1243812, 20, 20, 8)\n",
      "x_lstm_test (1243812, 12, 8)\n",
      "y_test (1243812, 33)\n",
      "x_cnn_val (3389388, 20, 20, 8)\n",
      "x_lstm_val (3389388, 12, 8)\n",
      "y_val (3389388, 33)\n"
     ]
    }
   ],
   "source": [
    "# 输出形状\n",
    "print('x_cnn_train_split',x_cnn_train_split.shape)\n",
    "print('x_lstm_train_split',x_lstm_train_split.shape)\n",
    "print('y_train_split',y_train_split.shape)\n",
    "\n",
    "print('x_cnn_test',x_cnn_test.shape)\n",
    "print('x_lstm_test',x_lstm_test.shape)\n",
    "print('y_test',y_test.shape)\n",
    "\n",
    "print('x_cnn_val',x_cnn_val.shape)\n",
    "print('x_lstm_val',x_lstm_val.shape)\n",
    "print('y_val',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d445bc",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39126622",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf565eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T07:44:55.435648Z",
     "start_time": "2025-05-18T07:44:55.427317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "932afe4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:39.570301Z",
     "start_time": "2024-05-09T03:27:39.561316Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   masked_softmax\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量，valid_lens:1D或2D张量\n",
    "    if valid_lens is None:\n",
    "        return tf.nn.softmax(X, axis=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:  # valid_lens 是1D的\n",
    "            valid_lens = tf.repeat(valid_lens, repeats=shape[1])    # 例如shape[1] = 2   [1,2,3]  --> [1,1,2,2,3,3]\n",
    "        else:\n",
    "            valid_lens = tf.reshape(valid_lens, shape=-1)   # 二维展平为一维\n",
    "            \n",
    "        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "        X = d2l.sequence_mask(tf.reshape(X, shape=(-1, shape[-1])), valid_lens, value=-1e6)\n",
    "        \n",
    "        return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "976a0b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:39.693104Z",
     "start_time": "2024-05-09T03:27:39.687113Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save 形状转化\n",
    "def transpose_qkv(X, num_heads):\n",
    "\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)\n",
    "\n",
    "    X = tf.reshape(X,shape=(-1, X.shape[1], num_heads, int(X.shape[2]/num_heads)))\n",
    " \n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
    "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))\n",
    "\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = tf.reshape(X, shape=(-1, num_heads, X.shape[1], X.shape[2]))\n",
    "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
    "    return tf.reshape(X, shape=(-1, X.shape[1],X.shape[2]*X.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0542b696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:39.813910Z",
     "start_time": "2024-05-09T03:27:39.807919Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   缩放点击注意力\n",
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Scaleddotproductattention.\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    # queries的形状：(batch_size，查询的个数，d)\n",
    "    # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "    # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "    \n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        d = queries.shape[-1]\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True)/tf.math.sqrt(tf.cast(d, dtype=tf.float32))\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)   # 返回加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d46f14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:39.935892Z",
     "start_time": "2024-05-09T03:27:39.927727Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   多头注意力\n",
    "class MultiHeadAttention_my(tf.keras.layers.Layer):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_size = key_size\n",
    "        self.query_size = query_size\n",
    "        self.value_size = value_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=bias)  # 全连接层\n",
    "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
    "        self.W_v = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
    "        self.W_o = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n",
    "       \n",
    "\n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        ''' \n",
    "        queries，keys，values的形状:\n",
    "        (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        \n",
    "        valid_lens　的形状:\n",
    "        (batch_size，)或(batch_size，查询的个数)\n",
    "        \n",
    "        经过变换后，输出的 queries，keys，values 的形状:\n",
    "        (batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        '''\n",
    "       \n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)  # 经过全连接后，进行形状转换，方便多头注意力的并行计算 \n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        \n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = tf.repeat(valid_lens, repeats=self.num_heads, axis=0)\n",
    "        \n",
    "        # output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)\n",
    "        \n",
    "        output = self.attention(queries, keys, values, valid_lens, **kwargs)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'num_heads': self.num_heads,\n",
    "                'key_size': self.key_size,\n",
    "                'query_size': self.query_size,\n",
    "                'value_size': self.value_size,\n",
    "                'num_hiddens': self.num_hiddens,\n",
    "                'dropout': self.dropout,\n",
    "                'bias': self.bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b80049cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:40.056519Z",
     "start_time": "2024-05-09T03:27:40.051527Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   基于位置的前馈网络\n",
    "class PositionWiseFFN(tf.keras.layers.Layer):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.ffn_num_hiddens = ffn_num_hiddens\n",
    "        self.ffn_num_outputs = ffn_num_outputs\n",
    "        self.dense1 = tf.keras.layers.Dense(ffn_num_hiddens)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dense2 = tf.keras.layers.Dense(ffn_num_outputs)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'ffn_num_hiddens': self.ffn_num_hiddens,\n",
    "                'ffn_num_outputs': self.ffn_num_outputs\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e695261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:40.178323Z",
     "start_time": "2024-05-09T03:27:40.173331Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   残差连接后进行规范化\n",
    "class AddNorm(tf.keras.layers.Layer):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.dropout = dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.ln = tf.keras.layers.LayerNormalization(normalized_shape)   # 沿着通道维度进行归一化\n",
    "\n",
    "    def call(self, X, Y, **kwargs):\n",
    "        return self.ln(self.dropout(Y, **kwargs) + X)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'normalized_shape': self.normalized_shape,\n",
    "                'dropout': self.dropout\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "75ef0392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:40.303123Z",
     "start_time": "2024-05-09T03:27:40.296134Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   位置编码\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        self.P = np.zeros((1, max_len, num_hiddens))\n",
    "        X = np.arange(max_len, dtype=np.float32).reshape(\n",
    "            -1,1)/np.power(10000, np.arange(\n",
    "            0, num_hiddens, 2, dtype=np.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = np.sin(X)\n",
    "        self.P[:, :, 1::2] = np.cos(X)\n",
    "\n",
    "    def call(self, X, **kwargs):\n",
    "        X = X + self.P[:, :X.shape[1], :]\n",
    "        return self.dropout(X, **kwargs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'num_hiddens': self.num_hiddens,\n",
    "                'dropout': self.dropout\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dcb0ebf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:40.398996Z",
     "start_time": "2024-05-09T03:27:40.386988Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#通道注意力       \n",
    "\n",
    "# class ChannelAttention(tf.keras.layers.Layer):     # 修改部分\n",
    "#     def __init__(self, input_channels):\n",
    "#         super(ChannelAttention, self).__init__()\n",
    "#         # 使用一个简单的全连接层来学习每个通道的重要性\n",
    "#         self.input_channels = input_channels\n",
    "#         self.fc = tf.keras.layers.Dense(input_channels, activation='relu')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # x的形状: (batch_size, channels)\n",
    "#         attention_weights = self.fc(x)  # 生成通道权重\n",
    "#         attention_weights = tf.keras.layers.Activation('softmax')(attention_weights)  # 归一化权重\n",
    "#         weighted_output = x * attention_weights  # 加权输入\n",
    "#         return weighted_output\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return{'input_channels': self.input_channels}\n",
    "    \n",
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        # 使用一个简单的全连接层来学习每个通道的重要性\n",
    "        self.input_channels = input_channels\n",
    "        self.fc1 = layers.Dense(input_channels//4, kernel_initializer='he_normal',\n",
    "                                kernel_regularizer=regularizers.l2(5e-4),\n",
    "                                activation=tf.nn.relu,\n",
    "                                use_bias=True, bias_initializer='zeros')\n",
    "        self.fc2 = layers.Dense(input_channels, kernel_initializer='he_normal',\n",
    "                                kernel_regularizer=regularizers.l2(5e-4),\n",
    "                                use_bias=True, bias_initializer='zeros')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x的形状: (batch_size, channels)\n",
    "        attention_weights = self.fc2(self.fc1(x))  # 生成通道权重\n",
    "        attention_weights = tf.keras.layers.Activation('softmax')(attention_weights)  # 归一化权重\n",
    "        weighted_output = x * attention_weights  # 加权输入\n",
    "        return weighted_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'input_channels': self.input_channels}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "64eb1e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:40.605636Z",
     "start_time": "2024-05-09T03:27:40.598647Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##   时序特征增强块\n",
    "\n",
    "def time_back(model_input):\n",
    "    model1 = tf.keras.layers.Conv1D(32, 3,padding='causal')(model_input)\n",
    "    model1 = tf.keras.layers.ReLU()(model1)\n",
    "    model1 = Dropout(rate=0.2)(model1)\n",
    "    \n",
    "    model2 = tf.keras.layers.Conv1D(32, 3,padding='causal',dilation_rate=2)(model_input)\n",
    "    model2 = tf.keras.layers.ReLU()(model2)\n",
    "    model2 = Dropout(rate=0.2)(model2)\n",
    "    \n",
    "    model3 = tf.keras.layers.Conv1D(32,1)(model_input)\n",
    "    \n",
    "    model_cat = tf.keras.layers.Add()([model1, model2])\n",
    "\n",
    "    model_lstm = tf.keras.layers.Bidirectional(LSTM(16, return_sequences=True))(model_cat) \n",
    "    model_lstm = tf.keras.layers.ReLU()(model_lstm)\n",
    "    model_lstm = Dropout(rate=0.2)(model_lstm)\n",
    "    \n",
    "    model_output =  tf.keras.layers.Add()([model_lstm, model3])\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "91e0c243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:42.671181Z",
     "start_time": "2024-05-09T03:27:42.663194Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##   Transformer块\n",
    "#@save    编码器块\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.key_size = key_size\n",
    "        self.query_size = query_size\n",
    "        self.value_size = value_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.norm_shape = norm_shape\n",
    "        self.ffn_num_hiddens = ffn_num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.attention = MultiHeadAttention_my(key_size, query_size, value_size, num_hiddens,\n",
    "                                                num_heads, dropout, bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)   #ffn_num_hiddens隐藏维度，num_hiddens输出维度\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def call(self, X, valid_lens, **kwargs):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens, **kwargs), **kwargs)\n",
    "        return self.addnorm2(Y, self.ffn(Y), **kwargs)\n",
    "    def get_config(self):\n",
    "        return{\n",
    "                'key_size': self.key_size,\n",
    "                'query_size': self.query_size,\n",
    "                'value_size': self.value_size,\n",
    "                'num_hiddens': self.num_hiddens,\n",
    "                'norm_shape': self.norm_shape,\n",
    "                'ffn_num_hiddens': self.ffn_num_hiddens,\n",
    "                'num_heads': self.num_heads,\n",
    "                'dropout': self.dropout,\n",
    "                'bias': self.bias,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf962038",
   "metadata": {},
   "source": [
    "## 空间 transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07b9cd9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:42.994663Z",
     "start_time": "2024-05-09T03:27:42.980707Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#  通道注意力 #\n",
    "class ChannelAttention_CNN(layers.Layer):\n",
    "    def __init__(self, in_planes, ratio=2):  #修改部分：由2修改为了4\n",
    "        super(ChannelAttention_CNN, self).__init__()\n",
    "        self.avg_out= layers.GlobalAveragePooling2D()\n",
    "        self.max_out= layers.GlobalMaxPooling2D()\n",
    "\n",
    "        self.fc1 = layers.Dense(in_planes//ratio, kernel_initializer='he_normal',\n",
    "                                kernel_regularizer=regularizers.l2(5e-4),\n",
    "                                activation=tf.nn.relu,use_bias=True, bias_initializer='zeros')\n",
    "        \n",
    "        self.fc2 = layers.Dense(in_planes, kernel_initializer='he_normal',\n",
    "                                kernel_regularizer=regularizers.l2(5e-4),\n",
    "                                use_bias=True, bias_initializer='zeros')\n",
    "\n",
    "    def call(self,inputs):\n",
    "        avg_out = self.avg_out(inputs)               #全局平均池化层\n",
    "        max_out = self.max_out(inputs)               #全局最大池化层\n",
    "        out = tf.stack([avg_out, max_out], axis=1)   #shape=(None, 2, fea_num)\n",
    "        out = self.fc2(self.fc1(out))                #多层感知机\n",
    "        out = tf.reduce_sum(out, axis=1)             # shape=(256, 512)\n",
    "        out = tf.nn.sigmoid(out)\n",
    "        out = layers.Reshape((1, 1, out.shape[1]))(out)\n",
    "        return out*inputs\n",
    "    \n",
    "    \n",
    "# 空间注意力 #\n",
    "def regularized_padded_conv(*args, **kwargs):   # *args：可变位置参数, **kwargs：可变关键字参数\n",
    "    return layers.Conv2D(*args, **kwargs, padding='same', use_bias=False,\n",
    "                         kernel_initializer='he_normal',\n",
    "                         kernel_regularizer=regularizers.l2(5e-4))\n",
    "\n",
    "class SpatialAttention_CNN(layers.Layer):\n",
    "    \n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention_CNN, self).__init__()\n",
    "        self.conv1 = regularized_padded_conv(1, kernel_size=kernel_size, strides=1, activation='sigmoid')  \n",
    "        # 输出通道数为1，卷积核大小7*7\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        avg_out = tf.reduce_mean(inputs, axis=3)                # 平均池化\n",
    "        max_out = tf.reduce_max(inputs, axis=3)                 # 最大池化\n",
    "        out = tf.stack([avg_out, max_out], axis=-1)             # 创建一个维度,拼接到一起concat。\n",
    "        out = self.conv1(out)\n",
    "        return out*inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save 形状转化 [方便注意力并行计算] \n",
    "def transpose_input_cnn(X, num_heads):\n",
    "\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，长，宽，通道)\n",
    "    # 输出X的形状:(batch_size，长，宽，num_heads，通道/num_heads)\n",
    "    X = tf.reshape(X,shape=(-1, X.shape[1], X.shape[2], num_heads, int(X.shape[3]/num_heads)))\n",
    "    #print('转化完成')\n",
    "    #print(X.shape)\n",
    "    # 输出X的形状:(batch_size，num_heads，长，宽，通道/num_heads)\n",
    "    X = tf.transpose(X, perm=(0, 3, 1, 2, 4))\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,长，宽，通道/num_heads)\n",
    "    return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3],X.shape[4]))\n",
    "#@save\n",
    "def transpose_output_cnn(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    #(batch_size*num_heads,长，宽，通道/num_heads) ==>(batch_size，num_heads,长，宽，通道/num_heads) \n",
    "    X = tf.reshape(X, shape=(-1, num_heads, X.shape[1], X.shape[2], X.shape[3]))\n",
    "    #print(X.shape)\n",
    "    # 输出形状为 (batch_size，长，宽，num_heads，通道/num_heads) \n",
    "    X = tf.transpose(X, perm=(0, 2, 3, 1, 4))\n",
    "    #print(X.shape)\n",
    "    return tf.reshape(X, shape=(-1, X.shape[1],X.shape[2],X.shape[3]*X.shape[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "680866e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:43.075568Z",
     "start_time": "2024-05-09T03:27:43.067543Z"
    },
    "code_folding": [
     30
    ]
   },
   "outputs": [],
   "source": [
    "#@save   卷积通道多头注意力\n",
    "class MultiHeadAttention_my_ChannelAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self,input_size, num_hiddens,num_heads, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.ChannelAttention_CNN = ChannelAttention_CNN(int(self.num_hiddens//self.num_heads))  # 输入通道数\n",
    "        \n",
    "        self.conv1 = Conv2D(filters=num_hiddens, kernel_size=(1, 1),strides=1,padding='same')   \n",
    "        self.conv2 = Conv2D(filters=num_hiddens, kernel_size=(1, 1),strides=1,padding='same')\n",
    "\n",
    "    def call(self, input_data, **kwargs):\n",
    "        # 1.先经过卷积层，并修改维度\n",
    "        queries = transpose_input_cnn(self.conv1(input_data),self.num_heads)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，长，宽，num_hiddens/num_heads)\n",
    "        # 2.进入注意力层\n",
    "        output = self.ChannelAttention_CNN(queries)  \n",
    "        \n",
    "        # 3.将形状修改回来\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output_cnn(output, self.num_heads)\n",
    "        \n",
    "        #4.通过最后的卷积层，并返回\n",
    "        return self.conv2(output_concat)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'input_size': self.input_size,\n",
    "                'num_hiddens': self.num_hiddens,\n",
    "                'num_heads': self.num_heads,\n",
    "                'dropout': self.dropout,}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad191771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:43.150410Z",
     "start_time": "2024-05-09T03:27:43.143421Z"
    },
    "code_folding": [
     24
    ]
   },
   "outputs": [],
   "source": [
    "#@save   卷积空间多头注意力\n",
    "class MultiHeadAttention_my_SpatialAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self,input_size, num_hiddens,num_heads, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.SpatialAttention_CNN =SpatialAttention_CNN()\n",
    "\n",
    "        self.conv1 = Conv2D(filters=num_hiddens, kernel_size=(1, 1),strides=1,padding='same')   # 首先经过1*1和3*3 来捕获通道和空间特征\n",
    "        self.conv2 = Conv2D(filters=num_hiddens, kernel_size=(1, 1),strides=1,padding='same')\n",
    "\n",
    "    def call(self, input_data, **kwargs):\n",
    "        # 1.先经过卷积层，并修改形状\n",
    "        queries = transpose_input_cnn(self.conv1(input_data),self.num_heads)\n",
    "         # 2.进入注意力层\n",
    "        output = self.SpatialAttention_CNN(queries)  \n",
    "         # 3.将形状修改回来\n",
    "        output_concat = transpose_output_cnn(output, self.num_heads)\n",
    "        #4.通过最后的卷积层，并返回\n",
    "        return self.conv2(output_concat)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'input_size': self.input_size,\n",
    "                'num_hiddens': self.num_hiddens,\n",
    "                'num_heads': self.num_heads,\n",
    "                'dropout': self.dropout,\n",
    "               }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e9c95837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:43.322134Z",
     "start_time": "2024-05-09T03:27:43.315146Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a09330d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:43.516821Z",
     "start_time": "2024-05-09T03:27:43.510831Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 深度可分离卷积\n",
    "class DepthwiseSeparableConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # 深度可分离卷积包括深度卷积和逐点卷积两个步骤\n",
    "        self.depthwise = tf.keras.layers.DepthwiseConv2D(kernel_size=3, padding='same',strides=1, depth_multiplier=1)\n",
    "        self.pointwise = tf.keras.layers.Conv2D(out_channels, kernel_size=1, padding='valid', strides=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "    def get_config(self):\n",
    "        return{'in_channels': self.in_channels,\n",
    "                'out_channels': self.out_channels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7706b452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:43.685549Z",
     "start_time": "2024-05-09T03:27:43.679559Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   卷积前馈网络\n",
    "class PositionWiseFFNCnn(tf.keras.layers.Layer):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.ffn_num_hiddens = ffn_num_hiddens\n",
    "        self.ffn_num_outputs = ffn_num_outputs\n",
    "        self.conv1 = Conv2D(filters=ffn_num_hiddens, kernel_size=(1, 1),strides=1,padding='same')\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        #self.conv2 = DepthwiseSeparableConv(in_channels=ffn_num_hiddens, out_channels=ffn_num_outputs)\n",
    "        self.conv2 = tf.keras.layers.SeparableConv2D(ffn_num_outputs, (3, 3), padding='same')   # 修改部分\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.conv2(self.relu(self.conv1(X)))\n",
    "    def get_config(self):\n",
    "        return{'ffn_num_hiddens': self.ffn_num_hiddens,\n",
    "                'ffn_num_outputs': self.ffn_num_outputs\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6768d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:45.117398Z",
     "start_time": "2024-05-09T03:27:45.111407Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#@save   残差连接后进行规范化\n",
    "class AddNormCnn(tf.keras.layers.Layer):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.ln = tf.keras.layers.LayerNormalization(normalized_shape)   # 沿着通道维度进行归一化\n",
    "\n",
    "    def call(self, X, Y, **kwargs):\n",
    "        return self.ln(self.dropout(Y, **kwargs) + X)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return{'normalized_shape': self.normalized_shape,\n",
    "                'dropout': self.dropout\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb36341b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:45.249186Z",
     "start_time": "2024-05-09T03:27:45.242197Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# CNN_transformer_Block\n",
    "class EncoderBlockCnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_size, num_hiddens,norm_shape,ffn_num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.ffn_num_hiddens = ffn_num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.norm_shape = norm_shape\n",
    "        \n",
    "        self.mca = MultiHeadAttention_my_ChannelAttention(input_size,num_hiddens,num_heads,dropout)\n",
    "        self.msa = MultiHeadAttention_my_SpatialAttention(input_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm1 = AddNormCnn(norm_shape,dropout)\n",
    "        self.ffn = PositionWiseFFNCnn(ffn_num_hiddens,num_hiddens)  # 隐藏层，输出层\n",
    "        self.addnorm2 = AddNormCnn(norm_shape,dropout)\n",
    "        \n",
    "    def call(self,X,**kwargs):\n",
    "        Y = self.addnorm1(X, self.msa(self.mca(X)), **kwargs)\n",
    "        return self.addnorm2(Y, self.ffn(Y), **kwargs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'input_size': self.input_size,\n",
    "                'num_hiddens': self.num_hiddens,\n",
    "                'norm_shape': self.norm_shape,\n",
    "                'ffn_num_hiddens': self.ffn_num_hiddens,\n",
    "                'num_heads': self.num_heads,\n",
    "                'dropout': self.dropout\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8db95e90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:45.370990Z",
     "start_time": "2024-05-09T03:27:45.364999Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 空间特征增强块\n",
    "def SpaceEnhanceBlock(model_input):\n",
    "    x = Conv2D(filters=16, kernel_size=(1, 5),strides=1,padding='same')(model_input)\n",
    "    x = BatchNormalization()(x) \n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x1 = Conv2D(filters=16, kernel_size=(5, 1),strides=1,dilation_rate=1,padding='same')(model_input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    \n",
    "    x3 = Conv2D(filters=32, kernel_size=(1, 1),strides=1,padding='same')(model_input)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    \n",
    "    x_cat = tf.keras.layers.Add()([x,x1])\n",
    "    x_cat = Activation('relu')(x_cat)\n",
    "    #x4 = DepthwiseSeparableConv(in_channels=16, out_channels=32)(x_cat)     \n",
    "    x4 = tf.keras.layers.SeparableConv2D(32, (3, 3), padding='same')(x_cat)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Activation('relu')(x4)\n",
    "\n",
    "    \n",
    "    out = tf.keras.layers.Add()([x3,x4])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1593f97",
   "metadata": {},
   "source": [
    "## 整体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3052724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.226253Z",
     "start_time": "2024-05-09T03:16:15.226253Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#整体模型     尝试修改\n",
    "def my_model():\n",
    "    # ==========================时间序列部分=============================================#\n",
    "    model_input= Input(shape=(12,8))\n",
    "    \n",
    "    #1、时序特征增强块\n",
    "    model = time_back(model_input) \n",
    "    \n",
    "    # 2、进行位置编码\n",
    "    # 因为位置编码值在-1和1之间，因此嵌入值乘以嵌入维度的平方根进行缩放，然后再与位置编码相加。\n",
    "    model = PositionalEncoding(32,0.2)(model * tf.math.sqrt(tf.cast(32, dtype=tf.float32)))\n",
    "    \n",
    "    # 3、transformer块\n",
    "    for _ in range(4): \n",
    "        model = EncoderBlock(32, 32, 32, 32, 2, 64, 8, 0.2)(model,None) # k,q,v,输出维度，层归一化维度，隐藏维度,注意力头数，dropout\n",
    "        model = Dropout(0.2)(model)\n",
    "    \n",
    "    # 4、得到输出 \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(64,activation='relu')(model)\n",
    "    \n",
    "    model_channel = ChannelAttention(64)(model)\n",
    "    out1 = tf.keras.layers.Add()([model, model_channel])\n",
    "    #model = tf.keras.layers.ReLU()(model_cat)\n",
    "    \n",
    "    #out1 = Dense(33)(model)\n",
    "\n",
    "    # ===============================空间部分============================================#\n",
    "    model2_input= Input(shape=(20,20,8))\n",
    "   \n",
    "    #1. 空间特征增强块\n",
    "    model2 = SpaceEnhanceBlock(model2_input)\n",
    "    \n",
    "    #2. 最大池化\n",
    "    model2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(model2)\n",
    "    \n",
    "    #3. CNN_transformer块\n",
    "    model2 = EncoderBlockCnn(32,32, -1, 32, 4, 0.2)(model2) \n",
    "    # 输入维度，输出维度，层归一化维度，卷积前馈神经网络隐藏层网络维度，注意力头数，dropout\n",
    "    model2 = Dropout(0.2)(model2)\n",
    "    \n",
    "    #4. 得到输出  \n",
    "    model2 = Conv2D(filters=32, kernel_size=(3,3),strides=1)(model2)\n",
    "    model2 = BatchNormalization()(model2)\n",
    "    model2= ReLU()(model2)\n",
    "    model2 = Conv2D(filters=64, kernel_size=(3,3),strides=1)(model2) \n",
    "    out2 = layers.GlobalAveragePooling2D()(model2)\n",
    "#     model2 = Flatten()(model2)\n",
    "#     out2 = Dense(33)(model2)\n",
    "    \n",
    "    # ==========================模型融合=========================================================#\n",
    "    ensemble = Concatenate()([out1, out2])\n",
    "    ensemble_out = Dense(128,activation='relu')(ensemble) \n",
    "    ensemble_out = Dropout(0.2)(ensemble_out)  # 添加Dropout层\n",
    "    \n",
    "    ensemble_channel = ChannelAttention(128)(ensemble_out)  # 经过注意力层\n",
    "    ensemble_cat = tf.keras.layers.Add()([ensemble_out, ensemble_channel])\n",
    "    \n",
    "    final_output = Dense(33)(ensemble_cat)\n",
    "    model = Model(inputs=[model_input,model2_input], outputs=final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81293af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.230246Z",
     "start_time": "2024-05-09T03:16:15.230246Z"
    }
   },
   "outputs": [],
   "source": [
    "model = my_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a1662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.231245Z",
     "start_time": "2024-05-09T03:16:15.231245Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8924bf",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c4794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.231245Z",
     "start_time": "2024-05-09T03:16:15.231245Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_all_model(x_train,y_train,x_val,y_val,name,model):\n",
    "    # 创建网络\n",
    "    #model = my_model(heads)`\n",
    "    # 编译网络\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    checkpoint_path=name\n",
    "    keras_callbacks   = [\n",
    "          EarlyStopping(monitor='val_loss', patience=20, mode='min', min_delta=0.0001),\n",
    "          ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=False, mode='min')\n",
    "    ]\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_val,y_val),\n",
    "        epochs=100, batch_size=5120, verbose=2, callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b604ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.233242Z",
     "start_time": "2024-05-09T03:16:15.233242Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "model = train_all_model([x_lstm_train_split,x_cnn_train_split],y_train_split,[x_lstm_val,x_cnn_val],y_val,\n",
    "                             '../model/lstm_transformer/CNN_test/all/temp/all_temp_ys_1/all_temp_ys-{epoch:02d}-{val_loss:.4f}.h5',my_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bda096",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "74b34726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:27:51.920808Z",
     "start_time": "2024-05-09T03:27:51.910824Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 评估函数定义\n",
    "# 异常相关系数\n",
    "import math\n",
    "def acc(actual, predicted):\n",
    "    pred_avg = np.average(predicted)\n",
    "\n",
    "    act_avg = np.average(actual)\n",
    "    diff_pred = predicted - pred_avg\n",
    "    diff_act = actual - act_avg\n",
    "    numerator = np.mean(np.sum(diff_pred*diff_act, axis=0))\n",
    "    denominator = math.sqrt(np.mean(np.sum(diff_pred**2, axis=0)) * np.mean(np.sum(diff_act**2, axis=0)))\n",
    "    ret_val = numerator/denominator\n",
    "    return (100 * ret_val)\n",
    "\n",
    "\n",
    "# 查看损失\n",
    "def look_loss(history):\n",
    "    plt.figure(figsize=(6.3, 2.5), dpi = 100)\n",
    "    plt.plot(history.history['loss'], label='training data')\n",
    "    plt.plot(history.history['val_loss'], label='validation data')\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "def all_estimate(best_model,x_test,y_test):\n",
    "    testPred_1 = best_model.predict(x_test)\n",
    "    a = y_test\n",
    "    y_test_p = a.reshape(-1,1)\n",
    "    testPred_p = testPred_1.reshape(-1,1)\n",
    "\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(y_test_p,testPred_p))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "    print('acc:',acc(y_test_p,testPred_p))\n",
    "\n",
    "    r2 = r2_score(y_test_p,testPred_p)\n",
    "    print(\"R² score:\", r2)  \n",
    "    \n",
    "\n",
    "def respective_estimate(best_model,x_test,y_test):\n",
    "    re2_list = []\n",
    "    rmse_list = []\n",
    "    for i in range(6):\n",
    "        # 模型预测\n",
    "        testPred = best_model.predict(x_test[i:i+1])\n",
    "        a = y_test[i:i+1]\n",
    "        y_test_p = a.reshape(-1,1)\n",
    "        testPred_p = testPred.reshape(-1,1)\n",
    "        r2 = r2_score(y_test_p,testPred_p)\n",
    "        rmse = sqrt(mean_squared_error(y_test_p,testPred_p))\n",
    "        re2_list.append(r2)\n",
    "        rmse_list.append(rmse)    \n",
    "    return re2_list,rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d334cc",
   "metadata": {},
   "source": [
    "## 对指定层进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4c96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.236237Z",
     "start_time": "2024-05-09T03:16:15.236237Z"
    }
   },
   "outputs": [],
   "source": [
    "num = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0f755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.237235Z",
     "start_time": "2024-05-09T03:16:15.237235Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = 'D:/codeFile/jupyterDemo/3d_ts/CNN-LSTM/model/lstm_transformer/CNN_test/all/temp/all_temp_ys_1/'\n",
    "\n",
    "# 获取文件夹下所有项的名称\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "# 过滤出文件，排除文件夹\n",
    "file_names = [item for item in items if os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfebab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.237235Z",
     "start_time": "2024-05-09T03:16:15.237235Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for i in file_names:\n",
    "    best_model = tf.keras.models.load_model(f'../model/lstm_transformer/CNN_test/all/temp/all_temp_ys_1/{i}',\n",
    "                                            custom_objects={'MultiHeadAttention_my': MultiHeadAttention_my,\n",
    "                                                            'PositionalEncoding':PositionalEncoding,\n",
    "                                                           'EncoderBlock':EncoderBlock,\n",
    "                                                            'EncoderBlockCnn':EncoderBlockCnn,\n",
    "                                                           'ChannelAttention':ChannelAttention,\n",
    "                                                           'DepthwiseSeparableConv':DepthwiseSeparableConv})                                      \n",
    "    \n",
    "    print(f'mode={i}')\n",
    "    #testPred = best_model.predict(x_lstm_test)\n",
    "    testPred = best_model.predict([x_lstm_test,x_cnn_test])\n",
    "    print('R²：',f'{r2_score(y_test,testPred):.4f}')\n",
    "    print('RMSE：',f'{sqrt(mean_squared_error(y_test,testPred)):.4f}')\n",
    "    \n",
    "    testPred_unscaled = scaler_l.inverse_transform(testPred)\n",
    "    y_test_unscaled = scaler_l.inverse_transform(y_test)\n",
    "    print('反归一化RMSE：',f'{sqrt(mean_squared_error(y_test_unscaled,testPred_unscaled)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a8964743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:30:38.260926Z",
     "start_time": "2024-05-09T03:27:59.232275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode=120\n",
      "R²： 0.7052\n",
      "RMSE： 0.5500\n",
      "反归一化RMSE： 0.4407\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(f'../model/lstm_transformer/CNN_test/all/temp/all_temp_ys_1/all_temp_ys-86-0.2300.h5',\n",
    "                                            custom_objects={'MultiHeadAttention_my': MultiHeadAttention_my,\n",
    "                                                            'PositionalEncoding':PositionalEncoding,\n",
    "                                                           'EncoderBlock':EncoderBlock,\n",
    "                                                            'EncoderBlockCnn':EncoderBlockCnn,\n",
    "                                                           'ChannelAttention':ChannelAttention,\n",
    "                                                           'DepthwiseSeparableConv':DepthwiseSeparableConv})                                      \n",
    "    \n",
    "print(f'mode={i}')\n",
    "#testPred = best_model.predict(x_lstm_test)\n",
    "testPred = best_model.predict([x_lstm_test,x_cnn_test])\n",
    "print('R²：',f'{r2_score(y_test,testPred):.4f}')\n",
    "print('RMSE：',f'{sqrt(mean_squared_error(y_test,testPred)):.4f}')\n",
    "\n",
    "testPred_unscaled = scaler_l.inverse_transform(testPred)\n",
    "y_test_unscaled = scaler_l.inverse_transform(y_test)\n",
    "print('反归一化RMSE：',f'{sqrt(mean_squared_error(y_test_unscaled,testPred_unscaled)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6914b",
   "metadata": {},
   "source": [
    "## 对所有层进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.239232Z",
     "start_time": "2024-05-09T03:16:15.239232Z"
    }
   },
   "outputs": [],
   "source": [
    "testPred_unscaled_reshape_1 = testPred_unscaled.reshape(12,-1,33)\n",
    "y_test_unscaled_reshape_1 = y_test_unscaled.reshape(12,-1,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ab440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.240230Z",
     "start_time": "2024-05-09T03:16:15.240230Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 查看精度\n",
    "depth = [5,10,15,20,30,40,50,60,70,80,90,100,125,150,175,200,225,250,275,300,350,400,500,600,700,800,900,1000,1100,\n",
    "              1300,1500,1750,2000]\n",
    "\n",
    "for i in range(0,33):\n",
    "    #归一化结果\n",
    "    nrmse = sqrt(mean_squared_error(y_test[:,i:i+1],testPred[:,i:i+1]))\n",
    "    print('%.4f' % nrmse)\n",
    "    r2 = r2_score(y_test[:,i:i+1],testPred[:,i:i+1])\n",
    "    print(r2)  \n",
    "    # 反归一化结果\n",
    "    print(f'{r2_score(y_test_unscaled[:,i:i+1],testPred_unscaled[:,i:i+1]):.4f}')\n",
    "    print(f'{sqrt(mean_squared_error(y_test_unscaled[:,i:i+1],testPred_unscaled[:,i:i+1])):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895be99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.241229Z",
     "start_time": "2024-05-09T03:16:15.241229Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_unscaled_reshape_1.shape,testPred_unscaled_reshape_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb059700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.242227Z",
     "start_time": "2024-05-09T03:16:15.242227Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,33):\n",
    "    #print(depth[i])\n",
    "    #rmse = sqrt(mean_squared_error(y_test_unscaled_reshape_1[i],testPred_unscaled_reshape_1[i]))\n",
    "    #print('Test RMSE: %.3f' % rmse) \n",
    "    #print(f'{r2_score(y_test_unscaled_reshape_1[11,:,i],testPred_unscaled_reshape_1[11,:,i]):.4f}')\n",
    "    print(f'{sqrt(mean_squared_error(y_test_unscaled_reshape_1[11,:,i],testPred_unscaled_reshape_1[11,:,i])):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed2f37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.243225Z",
     "start_time": "2024-05-09T03:16:15.243225Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,33):\n",
    "    \n",
    "    #print(f'{r2_score(y_test_unscaled_reshape_1[i],testPred_unscaled_reshape_1[i]):.4f}')\n",
    "    #print(f'{sqrt(mean_squared_error(y_test_unscaled_reshape_1[0,:,i],testPred_unscaled_reshape_1[0,:,i])):.4f}')\n",
    "    #print(f'{r2_score(y_test_unscaled_reshape_1[11,:,i],testPred_unscaled_reshape_1[11,:,i]):.4f}')\n",
    "    print(f'{sqrt(mean_squared_error(y_test_unscaled_reshape_1[11,:,i],testPred_unscaled_reshape_1[11,:,i])):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed69f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8979d6a8",
   "metadata": {},
   "source": [
    "# 模型预测结果保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14f760",
   "metadata": {},
   "source": [
    "## 对预测结果进行反归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd5749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.243225Z",
     "start_time": "2024-05-09T03:16:15.243225Z"
    }
   },
   "outputs": [],
   "source": [
    "testPred_unscaled = scaler_l.inverse_transform(testPred)\n",
    "y_test_unscaled = scaler_l.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47344926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.244224Z",
     "start_time": "2024-05-09T03:16:15.244224Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sqrt(mean_squared_error(y_test_unscaled,testPred_unscaled)))\n",
    "r2_score(y_test_unscaled,testPred_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77502e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.245222Z",
     "start_time": "2024-05-09T03:16:15.245222Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(33):\n",
    "    print('R²：',r2_score(y_test_unscaled[:,i:i+1],result_unscaled[:,i:i+1]))  # 真实值，预测值\n",
    "    print(sqrt(mean_squared_error(y_test_unscaled[:,i:i+1],result_unscaled[:,i:i+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae646981",
   "metadata": {},
   "source": [
    "### 为反归一化之后的预测结果和真实值填充nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512384a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.246220Z",
     "start_time": "2024-05-09T03:16:15.246220Z"
    }
   },
   "outputs": [],
   "source": [
    "#nan_mask_cat = np.load(\"../data_test/nan_mask_cat/nan_mask_cat_abnormal.npy\")\n",
    "#print(nan_mask_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3ed27a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:41.759951Z",
     "start_time": "2024-05-09T03:57:41.446456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144000, 396)\n",
      "(144000, 396)\n"
     ]
    }
   ],
   "source": [
    "### 创建一个形状与nan_masks相同且全是nan的数组\n",
    "testPred_with_nan = np.full((144000,12*33), np.nan)  \n",
    "print(testPred_with_nan.shape)\n",
    "y_test_with_nan = np.full((144000,12*33), np.nan)  \n",
    "print(y_test_with_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "12db86cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:45.436127Z",
     "start_time": "2024-05-09T03:57:45.294355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 103651, 33)\n",
      "(103651, 12, 33)\n",
      "(103651, 396)\n",
      "(12, 103651, 33)\n",
      "(103651, 12, 33)\n",
      "(103651, 396)\n"
     ]
    }
   ],
   "source": [
    "# 修改形状\n",
    "testPred_unscaled_reshape = testPred_unscaled.reshape(12,-1,33)\n",
    "print(testPred_unscaled_reshape.shape)\n",
    "testPred_unscaled_reshape = np.transpose(testPred_unscaled_reshape,(1,0,2))\n",
    "print(testPred_unscaled_reshape.shape)\n",
    "testPred_unscaled_reshape = testPred_unscaled_reshape.reshape(-1,12*33)\n",
    "print(testPred_unscaled_reshape.shape)\n",
    "# ====================================================================== #\n",
    "y_test_unscaled_reshape = y_test_unscaled.reshape(12,-1,33)\n",
    "print(y_test_unscaled_reshape.shape)\n",
    "y_test_unscaled_reshape = np.transpose(y_test_unscaled_reshape,(1,0,2))\n",
    "print(y_test_unscaled_reshape.shape)\n",
    "y_test_unscaled_reshape = y_test_unscaled_reshape.reshape(-1,12*33)\n",
    "print(y_test_unscaled_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fbb8d01c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:47.172330Z",
     "start_time": "2024-05-09T03:57:46.817901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103651\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i,nan_mask in enumerate(nan_mask_cat):   # i从0开始  在nan_mask_cat中true代表有效值，False代表无效值\n",
    "\n",
    "    if (nan_mask == True): # true 代表没有nan，使用预测值进行填充\n",
    "        testPred_with_nan[i] = testPred_unscaled_reshape[num]\n",
    "        y_test_with_nan[i] = y_test_unscaled_reshape[num]\n",
    "        num = num + 1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f3164095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:50.882809Z",
     "start_time": "2024-05-09T03:57:50.877818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 240, 600, 33) (12, 240, 600, 33)\n"
     ]
    }
   ],
   "source": [
    "# 奖形状转换回来\n",
    "testPred_with_nan = testPred_with_nan.reshape(240, 600,12,33)   \n",
    "testPred_with_nan = np.transpose(testPred_with_nan,(2,0,1,3))  \n",
    "\n",
    "y_test_with_nan = y_test_with_nan.reshape(240, 600,12,33)\n",
    "y_test_with_nan = np.transpose(y_test_with_nan,(2,0,1,3))\n",
    "\n",
    "print(testPred_with_nan.shape,y_test_with_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2110cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.252211Z",
     "start_time": "2024-05-09T03:16:15.252211Z"
    }
   },
   "outputs": [],
   "source": [
    "testPred_with_nan = np.transpose(testPred_with_nan,(0,3,1,2))  \n",
    "y_test_with_nan = np.transpose(y_test_with_nan,(0,3,1,2))\n",
    "print(testPred_with_nan.shape,y_test_with_nan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7ae90",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a0ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.253209Z",
     "start_time": "2024-05-09T03:16:15.253209Z"
    }
   },
   "outputs": [],
   "source": [
    "lat = []\n",
    "lon = []\n",
    "time = []\n",
    "lon.append(data_ssw['lon'][480:1080].data)\n",
    "lat.append(data_ssh1['lat'][360:600].data)\n",
    "start_year = 2021\n",
    "end_year =2021\n",
    "time = []\n",
    "date_range = pd.date_range(datetime(start_year,1,1),datetime(end_year+1,1,1),freq='1M')\n",
    "len(date_range)\n",
    "for i in range(len(date_range)):\n",
    "    time.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae165b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.254207Z",
     "start_time": "2024-05-09T03:16:15.254207Z"
    }
   },
   "outputs": [],
   "source": [
    "new_NC = nc.Dataset(\"../model/lstm_transformer/result/temp/result_5_2000m_temp_ys.nc\", 'w', format='NETCDF4')\n",
    "\n",
    "new_NC.createDimension('lat', len(lat[0]))\n",
    "new_NC.createDimension('lon', len(lon[0]))\n",
    "new_NC.createDimension('depth', 33)\n",
    "new_NC.createDimension('time', len(time))\n",
    "\n",
    "new_NC.createVariable('lat', 'f', (\"lat\"))\n",
    "new_NC.createVariable('lon', 'f', (\"lon\"))\n",
    "new_NC.createVariable('depth', 'f', (\"depth\"))\n",
    "new_NC.createVariable('true_sta', 'f',(\"time\",\"depth\",\"lat\",\"lon\"))\n",
    "new_NC.createVariable('pred_sta', 'f',(\"time\",\"depth\",\"lat\",\"lon\"))\n",
    "\n",
    "time_var = new_NC.createVariable('time', 'f4',(\"time\"))\n",
    "time_var.units = 'months since 2021-01-15'\n",
    "time_var.long_name = 'Months in Monthly Means'\n",
    "time_var.axis = 'T'\n",
    "\n",
    "#向变量中填充数据\n",
    "new_NC.variables['lat'][:] = lat[0]\n",
    "new_NC.variables['lon'][:] = lon[0]\n",
    "new_NC.variables['time'][:] = np.array(time)\n",
    "new_NC.variables['depth'][:] = depth\n",
    "\n",
    "new_NC.variables['true_sta'][:]=np.array(y_test_with_nan)\n",
    "new_NC.variables['pred_sta'][:]=np.array(testPred_with_nan)\n",
    "\n",
    "#最后记得关闭文件\n",
    "new_NC.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8deb380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d3f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceb1ee72",
   "metadata": {},
   "source": [
    "### 将反归一化之后的温度异常加入，评估次表层温度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9cba5a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:55.742748Z",
     "start_time": "2024-05-09T03:57:55.737755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 638, 33)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli_3DT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e4d2aa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:57:56.251927Z",
     "start_time": "2024-05-09T03:57:56.110156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 278, 638, 33)\n"
     ]
    }
   ],
   "source": [
    "cli_3DT_exp = np.expand_dims(cli_3DT,axis=0)\n",
    "cli_3DT_exp = np.tile(cli_3DT_exp,(12,1,1,1))\n",
    "print(cli_3DT_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "91911e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:58:07.683708Z",
     "start_time": "2024-05-09T03:58:07.401163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 240, 600, 33)\n"
     ]
    }
   ],
   "source": [
    "pred_3dt  = testPred_with_nan + cli_3DT_exp[:,19:-19,19:-19]\n",
    "print(pred_3dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "57532cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:58:16.969077Z",
     "start_time": "2024-05-09T03:58:16.965084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 240, 600, 33)\n"
     ]
    }
   ],
   "source": [
    "temp_depth_use_5_2000 =  temp_depth_use[-12:,19:-19,19:-19]\n",
    "print(temp_depth_use_5_2000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ccf465c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:58:18.876080Z",
     "start_time": "2024-05-09T03:58:18.786225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728000, 33) (1728000, 33)\n"
     ]
    }
   ],
   "source": [
    "temp_depth_use_flatten = temp_depth_use_5_2000.reshape(-1,33)\n",
    "pred_3dt_flatten = pred_3dt.reshape(-1,33)\n",
    "print(temp_depth_use_flatten.shape,pred_3dt_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "514b4bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T04:04:26.222369Z",
     "start_time": "2024-05-09T04:04:23.172185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3435\n",
      "0.3479\n",
      "0.3666\n",
      "0.4125\n",
      "0.5237\n",
      "0.6058\n",
      "0.6573\n",
      "0.6812\n",
      "0.6878\n",
      "0.6831\n",
      "0.6720\n",
      "0.6696\n",
      "0.6759\n",
      "0.6782\n",
      "0.6098\n",
      "0.5125\n",
      "0.4438\n",
      "0.3874\n",
      "0.3563\n",
      "0.3354\n",
      "0.3194\n",
      "0.3023\n",
      "0.2561\n",
      "0.2004\n",
      "0.1460\n",
      "0.1095\n",
      "0.0896\n",
      "0.0778\n",
      "0.0708\n",
      "0.0587\n",
      "0.0505\n",
      "0.0195\n",
      "0.0296\n"
     ]
    }
   ],
   "source": [
    "for i in range(33): #  0,1,2\n",
    "    #print('层数',depth[i],'m')\n",
    "    pred_3dt_flatten_mask = np.isnan(pred_3dt_flatten[:,i:i+1])\n",
    "    pred_3dt_flatten_nonan = pred_3dt_flatten[:,i:i+1][~pred_3dt_flatten_mask[:, 0]]\n",
    "    temp_depth_use_flatten_nonan = temp_depth_use_flatten[:,i:i+1][~pred_3dt_flatten_mask[:, 0]]\n",
    "    \n",
    "    #print(f'{r2_score(temp_depth_use_flatten_nonan,pred_3dt_flatten_nonan):.4f}')  # 真实值，预测值\n",
    "    print(f'{sqrt(mean_squared_error(temp_depth_use_flatten_nonan,pred_3dt_flatten_nonan)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6d53a",
   "metadata": {},
   "source": [
    "## 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09355c39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.260198Z",
     "start_time": "2024-05-09T03:16:15.260198Z"
    }
   },
   "outputs": [],
   "source": [
    "end_temp_depth = np.transpose(temp_depth_use_5_2000,(0,3,1,2))\n",
    "end_pred_3dt =  np.transpose(pred_3dt,(0,3,1,2))\n",
    "print(end_temp_depth.shape,end_pred_3dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430968ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.261196Z",
     "start_time": "2024-05-09T03:16:15.261196Z"
    }
   },
   "outputs": [],
   "source": [
    "#depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ef9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.262195Z",
     "start_time": "2024-05-09T03:16:15.262195Z"
    }
   },
   "outputs": [],
   "source": [
    "lat = []\n",
    "lon = []\n",
    "time = []\n",
    "lon.append(data_ssw['lon'][480:1080].data)\n",
    "lat.append(data_ssh1['lat'][360:600].data)\n",
    "start_year = 2021\n",
    "end_year =2021\n",
    "time = []\n",
    "date_range = pd.date_range(datetime(start_year,1,1),datetime(end_year+1,1,1),freq='1M')\n",
    "len(date_range)\n",
    "for i in range(len(date_range)):\n",
    "    time.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e3e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.263193Z",
     "start_time": "2024-05-09T03:16:15.263193Z"
    }
   },
   "outputs": [],
   "source": [
    "new_NC.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ae538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T03:16:15.264191Z",
     "start_time": "2024-05-09T03:16:15.264191Z"
    }
   },
   "outputs": [],
   "source": [
    "new_NC = nc.Dataset(\"../model/lstm_transformer/result/temp/result5_2000m_cnn_20×20.nc\", 'w', format='NETCDF4')\n",
    "\n",
    "new_NC.createDimension('lat', len(lat[0]))\n",
    "new_NC.createDimension('lon', len(lon[0]))\n",
    "new_NC.createDimension('depth', 33)\n",
    "new_NC.createDimension('time', len(time))\n",
    "\n",
    "new_NC.createVariable('lat', 'f', (\"lat\"))\n",
    "new_NC.createVariable('lon', 'f', (\"lon\"))\n",
    "new_NC.createVariable('depth', 'f', (\"depth\"))\n",
    "new_NC.createVariable('true_sta', 'f',(\"time\",\"depth\",\"lat\",\"lon\"))\n",
    "new_NC.createVariable('pred_sta', 'f',(\"time\",\"depth\",\"lat\",\"lon\"))\n",
    "\n",
    "time_var = new_NC.createVariable('time', 'f4',(\"time\"))\n",
    "time_var.units = 'months since 2021-01-15'\n",
    "time_var.long_name = 'Months in Monthly Means'\n",
    "time_var.axis = 'T'\n",
    "\n",
    "#向变量中填充数据\n",
    "new_NC.variables['lat'][:] = lat[0]\n",
    "new_NC.variables['lon'][:] = lon[0]\n",
    "new_NC.variables['time'][:] = np.array(time)\n",
    "new_NC.variables['depth'][:] = depth\n",
    "\n",
    "new_NC.variables['true_sta'][:]=np.array(y_test_with_nan)\n",
    "new_NC.variables['pred_sta'][:]=np.array(testPred_with_nan)\n",
    "\n",
    "#最后记得关闭文件\n",
    "new_NC.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59545567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7c8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324.915px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
